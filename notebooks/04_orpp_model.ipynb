{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One row per patient model\n",
    "\n",
    "To keep it simple, to start off with we simply model the patients as a single row, i.e. each feature is a scalar or a categorical variable.\n",
    "\n",
    "See `03_mortality_red_dataset` notebook for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "establish connection to DB and define helper function for running queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from proto.etl.config import SSHInfoEicu, DBInfoEicu\n",
    "from proto.etl.utils import connect_to_db_via_ssh, run_eicu_query, get_column_completeness, load_schema_for_modelling\n",
    "\n",
    "conn = connect_to_db_via_ssh(SSHInfoEicu, DBInfoEicu)\n",
    "cursor = conn.cursor()\n",
    "query_schema = 'set search_path to eicu_crd;'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data, get target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, MinMaxScaler\n",
    "\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Dropout, concatenate, BatchNormalization, SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vars\n",
    "df = pd.read_csv('orpp_all.csv').set_index('patientunitstayid')\n",
    "\n",
    "# load targets\n",
    "query = \"\"\"\n",
    "select p.patientunitstayid, i.hosp_mort, icu_los_hours\n",
    "from patient_top5hospitals_mort_dataset p\n",
    "inner join icustay_detail i\n",
    "on p.patientunitstayid=i.patientunitstayid\n",
    "\"\"\"\n",
    "df_y = run_eicu_query(query, conn).set_index('patientunitstayid')\n",
    "\n",
    "# there are 100 missing mortality labels, we impute them with zero\n",
    "#re-order y vars to match the order of X\n",
    "df_y.fillna(0, inplace=True)\n",
    "df_y = df_y.loc[df.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup encoders for the categorical input vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoders = {\n",
    "    'ethnicity': LabelEncoder(),\n",
    "    'hospital_region': LabelEncoder(),\n",
    "    'unittype': LabelEncoder(),\n",
    "    'apachedxgroup': LabelEncoder()\n",
    "}\n",
    "for col, label_encoder in cat_encoders.items():\n",
    "    df[col] = label_encoder.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalise numeric input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = [\n",
    "    'ethnicity',\n",
    "    'hospital_region',\n",
    "    'unittype',\n",
    "    'apachedxgroup'\n",
    "]\n",
    "num_cols = list(df.columns[4:].values)\n",
    "\n",
    "# we don't want to scale the embed dims\n",
    "embed_dims = 100\n",
    "num_cols_to_scale = num_cols[:-embed_dims]\n",
    "scaler = RobustScaler(quantile_range=(10.0, 90.0))\n",
    "# scaler = MinMaxScaler()\n",
    "df[num_cols_to_scale] = scaler.fit_transform(df[num_cols_to_scale].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup training/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do train-test split without scikit to keep the data with its col names - see later\n",
    "np.random.seed(42)\n",
    "test_ratio = 0.1\n",
    "train_ix = np.random.rand(len(df)) < 1 - test_ratio\n",
    "\n",
    "# define X and y, then split it into train (90%) and test (10%)\n",
    "X = df.values\n",
    "y = df_y['hosp_mort'].values\n",
    "df_X_train = df[train_ix]\n",
    "y_train = y[train_ix]\n",
    "df_X_test = df[~train_ix]\n",
    "y_test = y[~train_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dict(df, num_cols):\n",
    "    return {\n",
    "        'ethnicity': np.array(df.ethnicity),\n",
    "        'hospital_region': np.array(df.hospital_region),\n",
    "        'unittype': np.array(df.unittype),\n",
    "        'apachedxgroup': np.array(df.apachedxgroup),\n",
    "        'num_cols': df[num_cols].values\n",
    "    }\n",
    "X_train = get_data_dict(df_X_train, num_cols)\n",
    "X_test = get_data_dict(df_X_test, num_cols)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define basic FFN model and its hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 5\n",
    "DENSE_SIZE = 512\n",
    "BATCH_SIZE = 2048\n",
    "EPOCHS = 100\n",
    "DROPOUT = 0.25\n",
    "STEPS = int(len(X_train) / BATCH_SIZE) * EPOCHS\n",
    "\n",
    "# set up decaying learning rate for Adam\n",
    "LR_INIT, LR_FIN = 0.001, 0.0001\n",
    "exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "lr_decay = exp_decay(LR_INIT, LR_FIN, STEPS)\n",
    "optimizer_adam = Adam(lr=0.001, decay=lr_decay, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/danielhomola/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "num_cols (InputLayer)           (None, 207)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ethnicity (InputLayer)          (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hospital_region (InputLayer)    (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "unittype (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "apachedxgroup (InputLayer)      (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          106496      num_cols[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 2)         14          ethnicity[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 2)         6           hospital_region[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 2)         16          unittype[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 5)         105         apachedxgroup[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1, 11)        0           embedding[0][0]                  \n",
      "                                                                 embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "                                                                 embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          131328      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 11)           0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 267)          0           flatten[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          34304       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            129         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 272,398\n",
      "Trainable params: 272,398\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# cat features embedded\n",
    "in_et = Input(shape=(1,), name='ethnicity')\n",
    "emb_et = Embedding(cat_encoders['ethnicity'].classes_.shape[0], 2)(in_et)\n",
    "in_hr = Input(shape=(1,), name='hospital_region')\n",
    "emb_hr = Embedding(cat_encoders['hospital_region'].classes_.shape[0], 2)(in_hr)\n",
    "in_ut = Input(shape=(1,), name='unittype')\n",
    "emb_ut = Embedding(cat_encoders['unittype'].classes_.shape[0], 2)(in_ut)\n",
    "in_ag = Input(shape=(1,), name='apachedxgroup')\n",
    "emb_ag = Embedding(cat_encoders['apachedxgroup'].classes_.shape[0], 5)(in_ag)\n",
    "\n",
    "cat_feats = concatenate([emb_et, emb_hr, emb_ut, emb_ag])\n",
    "cat_feats = Flatten()(cat_feats)\n",
    "\n",
    "# num features with 2 layers and dropout\n",
    "in_num = Input(shape=(len(num_cols),), name='num_cols')\n",
    "num_feats = Dense(DENSE_SIZE, activation='relu')(in_num)\n",
    "num_feats = Dropout(DROPOUT)(num_feats)\n",
    "num_feats = Dense(DENSE_SIZE/2, activation='relu')(num_feats)\n",
    "num_feats = Dropout(DROPOUT)(num_feats)\n",
    "\n",
    "# concat cat and num features add final dense layer, output layer and compile model\n",
    "all_feats = concatenate([cat_feats, num_feats])\n",
    "all_feats = Dense(DENSE_SIZE/4, activation='relu')(all_feats)\n",
    "all_feats = Dropout(DROPOUT)(all_feats)\n",
    "\n",
    "out = Dense(1, activation=None)(all_feats)\n",
    "model = Model(inputs=[in_et, in_hr, in_ut, in_ag, in_num], outputs=out)\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=optimizer_adam, \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 0s - loss: 7.8781 - acc: 0.9039\n",
      "Epoch 2/100\n",
      " - 0s - loss: 7.8736 - acc: 0.9039\n",
      "Epoch 3/100\n",
      " - 0s - loss: 7.8778 - acc: 0.9039\n",
      "Epoch 4/100\n",
      " - 0s - loss: 7.8767 - acc: 0.9039\n",
      "Epoch 5/100\n",
      " - 0s - loss: 7.8557 - acc: 0.9039\n",
      "Epoch 6/100\n",
      " - 0s - loss: 7.8763 - acc: 0.9039\n",
      "Epoch 7/100\n",
      " - 0s - loss: 7.8728 - acc: 0.9039\n",
      "Epoch 8/100\n",
      " - 0s - loss: 7.8754 - acc: 0.9039\n",
      "Epoch 9/100\n",
      " - 0s - loss: 7.8706 - acc: 0.9039\n",
      "Epoch 10/100\n",
      " - 0s - loss: 7.8765 - acc: 0.9039\n",
      "Epoch 11/100\n",
      " - 0s - loss: 7.8799 - acc: 0.9039\n",
      "Epoch 12/100\n",
      " - 0s - loss: 7.8739 - acc: 0.9039\n",
      "Epoch 13/100\n",
      " - 0s - loss: 7.8763 - acc: 0.9039\n",
      "Epoch 14/100\n",
      " - 0s - loss: 7.8728 - acc: 0.9039\n",
      "Epoch 15/100\n",
      " - 0s - loss: 7.8792 - acc: 0.9039\n",
      "Epoch 16/100\n",
      " - 0s - loss: 7.8723 - acc: 0.9039\n",
      "Epoch 17/100\n",
      " - 0s - loss: 7.8619 - acc: 0.9039\n",
      "Epoch 18/100\n",
      " - 0s - loss: 7.8671 - acc: 0.9039\n",
      "Epoch 19/100\n",
      " - 0s - loss: 7.8717 - acc: 0.9039\n",
      "Epoch 20/100\n",
      " - 0s - loss: 7.8655 - acc: 0.9039\n",
      "Epoch 21/100\n",
      " - 0s - loss: 7.8802 - acc: 0.9039\n",
      "Epoch 22/100\n",
      " - 0s - loss: 7.8735 - acc: 0.9039\n",
      "Epoch 23/100\n",
      " - 0s - loss: 7.8755 - acc: 0.9039\n",
      "Epoch 24/100\n",
      " - 0s - loss: 7.8754 - acc: 0.9039\n",
      "Epoch 25/100\n",
      " - 0s - loss: 7.8705 - acc: 0.9039\n",
      "Epoch 26/100\n",
      " - 0s - loss: 7.8730 - acc: 0.9039\n",
      "Epoch 27/100\n",
      " - 0s - loss: 7.8769 - acc: 0.9039\n",
      "Epoch 28/100\n",
      " - 0s - loss: 7.8750 - acc: 0.9039\n",
      "Epoch 29/100\n",
      " - 0s - loss: 7.8759 - acc: 0.9039\n",
      "Epoch 30/100\n",
      " - 0s - loss: 7.8744 - acc: 0.9039\n",
      "Epoch 31/100\n",
      " - 0s - loss: 7.8694 - acc: 0.9039\n",
      "Epoch 32/100\n",
      " - 0s - loss: 7.8749 - acc: 0.9039\n",
      "Epoch 33/100\n",
      " - 0s - loss: 7.8683 - acc: 0.9039\n",
      "Epoch 34/100\n",
      " - 0s - loss: 7.8696 - acc: 0.9039\n",
      "Epoch 35/100\n",
      " - 0s - loss: 7.8767 - acc: 0.9039\n",
      "Epoch 36/100\n",
      " - 0s - loss: 7.8752 - acc: 0.9039\n",
      "Epoch 37/100\n",
      " - 0s - loss: 7.8755 - acc: 0.9039\n",
      "Epoch 38/100\n",
      " - 0s - loss: 7.8714 - acc: 0.9039\n",
      "Epoch 39/100\n",
      " - 0s - loss: 7.8796 - acc: 0.9039\n",
      "Epoch 40/100\n",
      " - 0s - loss: 7.8685 - acc: 0.9039\n",
      "Epoch 41/100\n",
      " - 0s - loss: 7.8786 - acc: 0.9039\n",
      "Epoch 42/100\n",
      " - 0s - loss: 7.8664 - acc: 0.9039\n",
      "Epoch 43/100\n",
      " - 0s - loss: 7.8723 - acc: 0.9039\n",
      "Epoch 44/100\n",
      " - 0s - loss: 7.8733 - acc: 0.9039\n",
      "Epoch 45/100\n",
      " - 0s - loss: 7.8695 - acc: 0.9039\n",
      "Epoch 46/100\n",
      " - 0s - loss: 7.8686 - acc: 0.9039\n",
      "Epoch 47/100\n",
      " - 0s - loss: 7.8724 - acc: 0.9039\n",
      "Epoch 48/100\n",
      " - 0s - loss: 7.8745 - acc: 0.9039\n",
      "Epoch 49/100\n",
      " - 0s - loss: 7.8751 - acc: 0.9039\n",
      "Epoch 50/100\n",
      " - 0s - loss: 7.8678 - acc: 0.9039\n",
      "Epoch 51/100\n",
      " - 0s - loss: 7.8646 - acc: 0.9039\n",
      "Epoch 52/100\n",
      " - 0s - loss: 7.8693 - acc: 0.9039\n",
      "Epoch 53/100\n",
      " - 0s - loss: 7.8786 - acc: 0.9039\n",
      "Epoch 54/100\n",
      " - 0s - loss: 7.8796 - acc: 0.9039\n",
      "Epoch 55/100\n",
      " - 0s - loss: 7.8749 - acc: 0.9039\n",
      "Epoch 56/100\n",
      " - 0s - loss: 7.8737 - acc: 0.9039\n",
      "Epoch 57/100\n",
      " - 0s - loss: 7.8725 - acc: 0.9039\n",
      "Epoch 58/100\n",
      " - 0s - loss: 7.8761 - acc: 0.9039\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f2b7fcd876ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/.virtualenvs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, y_train, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    class_weight={0: 0.1, 1: 0.9},\n",
    "    epochs=EPOCHS, \n",
    "    shuffle=True, \n",
    "    verbose=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
