{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated mortality prediction\n",
    "\n",
    "We'll split the train data into two workers and train the model in a federated way using PySyft. \n",
    "\n",
    "This was adopted from one of the basic [PySyft tutorials](https://github.com/OpenMined/PySyft/blob/dev/examples/tutorials/Part%2004%20-%20Federated%20Learning%20via%20Trusted%20Aggregator.ipynb).\n",
    "\n",
    "To keep it really simple, we'll ignore the categorical variables and diag columns for now and just use the apache and lab values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import syft as sy\n",
    "import copy\n",
    "hook = sy.TorchHook(torch)\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import precision_recall_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('orpp_all.csv').set_index('patientunitstayid')\n",
    "# keep apache and lab values only - all numeric\n",
    "df = df.iloc[:, 4:107]\n",
    "# scale X\n",
    "scaler = RobustScaler(quantile_range=(10.0, 90.0))\n",
    "df = scaler.fit_transform(df.values)\n",
    "\n",
    "# load y\n",
    "df_y = pd.read_csv('mort_y.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup training/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do train-test split without scikit to keep the data with its col names - see later\n",
    "np.random.seed(12)\n",
    "test_ratio = 0.1\n",
    "train_ix = np.random.rand(len(df)) < 1 - test_ratio\n",
    "\n",
    "# define X and y, then split it into train (90%) and test (10%)\n",
    "y = df_y['hosp_mort'].values\n",
    "X_train = df[train_ix].astype('float32')\n",
    "y_train = y[train_ix].astype('float32')\n",
    "X_test = df[~train_ix].astype('float32')\n",
    "y_test = y[~train_ix].astype('float32')\n",
    "\n",
    "# now split training into 2 equal parts randomly - simulating two distinct datasets\n",
    "hosp1_ix = np.random.rand(X_train.shape[0]) < 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Owners\n",
    "\n",
    "First, we're going to create two data owners (Hospital1 and Hospital2), with the same amount of data. We're also going to initialize a secure machine called \"secure_worker\". In practice this could be secure hardware (such as Intel's SGX) or simply a trusted intermediary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a couple workers\n",
    "hosp1 = sy.VirtualWorker(hook, id=\"hospital1\")\n",
    "hosp2 = sy.VirtualWorker(hook, id=\"hospital2\")\n",
    "secure_worker = sy.VirtualWorker(hook, id=\"secure_worker\")\n",
    "\n",
    "# A Toy Dataset\n",
    "data = torch.tensor(X_train, requires_grad=False)\n",
    "target = torch.tensor(y_train, requires_grad=False)\n",
    "\n",
    "# get pointers to training data on each worker by sending some training data to hospital1 and 2\n",
    "hosp1_data = data[np.where(hosp1_ix)[0]].send(hosp1)\n",
    "hosp1_target = target[np.where(hosp1_ix)[0]].send(hosp1)\n",
    "hosp1_data_n = len(hosp1_data)\n",
    "\n",
    "hosp2_data = data[np.where(~hosp1_ix)[0]].send(hosp2)\n",
    "hosp2_target = target[np.where(~hosp1_ix)[0]].send(hosp2)\n",
    "hosp2_data_n = len(hosp2_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DNN model.\n",
    "\n",
    "Then send it to hospital 1 and 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DENSE_SIZE = 1024\n",
    "BATCH_SIZE = 1024\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(103, DENSE_SIZE)\n",
    "        self.fc2 = nn.Linear(DENSE_SIZE, int(DENSE_SIZE/2))\n",
    "        self.fc3 = nn.Linear(int(DENSE_SIZE/2), int(DENSE_SIZE/4))\n",
    "        self.fc4 = nn.Linear(int(DENSE_SIZE/4), 1)\n",
    "        self.do = nn.Dropout(p=DROPOUT)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.do(F.relu(self.fc1(x)))\n",
    "        x = self.do(F.relu(self.fc2(x)))\n",
    "        x = self.do(F.relu(self.fc3(x)))\n",
    "        x = self.sigm(self.fc4(x))\n",
    "        return x\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the federated model\n",
    "\n",
    "As is conventional with Federated Learning via Secure Averaging, each data owner first trains their model for several iterations locally before the models are averaged together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ix(batch, batch_size, total_size):\n",
    "    start = batch * batch_size\n",
    "    if start > total_size:\n",
    "        raise ValueError('We ran out of data captain!')\n",
    "    end = start + batch_size\n",
    "    if end > total_size:\n",
    "        end = total_size\n",
    "    return slice(start, end, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "GLOBAL EPOCH 0\n",
      "LOCAL EPOCH 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danielhomola/.virtualenvs/tf/lib/python3.6/site-packages/syft/frameworks/torch/tensors/interpreters/native.py:308: UserWarning: Using a target size (torch.Size([1024])) that is different to the input size (torch.Size([1024, 1])) is deprecated. Please ensure they have the same size.\n",
      "  response = eval(cmd)(*args, **kwargs)\n",
      "/home/danielhomola/.virtualenvs/tf/lib/python3.6/site-packages/syft/frameworks/torch/tensors/interpreters/native.py:308: UserWarning: Using a target size (torch.Size([223])) that is different to the input size (torch.Size([223, 1])) is deprecated. Please ensure they have the same size.\n",
      "  response = eval(cmd)(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hospital 1 loss:tensor(0.5909)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danielhomola/.virtualenvs/tf/lib/python3.6/site-packages/syft/frameworks/torch/tensors/interpreters/native.py:308: UserWarning: Using a target size (torch.Size([356])) that is different to the input size (torch.Size([356, 1])) is deprecated. Please ensure they have the same size.\n",
      "  response = eval(cmd)(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hospital 2 loss:tensor(0.6178)\n",
      "LOCAL EPOCH 1\n",
      "Hospital 1 loss:tensor(0.5386)\n",
      "Hospital 2 loss:tensor(0.5212)\n",
      "LOCAL EPOCH 2\n",
      "Hospital 1 loss:tensor(0.5234)\n",
      "Hospital 2 loss:tensor(0.5366)\n",
      "LOCAL EPOCH 3\n",
      "Hospital 1 loss:tensor(0.4647)\n",
      "Hospital 2 loss:tensor(0.4632)\n",
      "LOCAL EPOCH 4\n",
      "Hospital 1 loss:tensor(0.4647)\n",
      "Hospital 2 loss:tensor(0.4466)\n",
      "Hospital1:tensor(0.4466) Hospital2:tensor(0.4466)\n",
      "------------------------------------\n",
      "GLOBAL EPOCH 1\n",
      "LOCAL EPOCH 0\n",
      "Hospital 1 loss:tensor(0.4321)\n",
      "Hospital 2 loss:tensor(0.4142)\n",
      "LOCAL EPOCH 1\n",
      "Hospital 1 loss:tensor(0.3823)\n",
      "Hospital 2 loss:tensor(0.4416)\n",
      "LOCAL EPOCH 2\n",
      "Hospital 1 loss:tensor(0.3797)\n",
      "Hospital 2 loss:tensor(0.4108)\n",
      "LOCAL EPOCH 3\n",
      "Hospital 1 loss:tensor(0.3579)\n",
      "Hospital 2 loss:tensor(0.3698)\n",
      "LOCAL EPOCH 4\n",
      "Hospital 1 loss:tensor(0.3221)\n",
      "Hospital 2 loss:tensor(0.3464)\n",
      "Hospital1:tensor(0.3464) Hospital2:tensor(0.3464)\n",
      "------------------------------------\n",
      "GLOBAL EPOCH 2\n",
      "LOCAL EPOCH 0\n",
      "Hospital 1 loss:tensor(0.3464)\n",
      "Hospital 2 loss:tensor(0.3428)\n",
      "LOCAL EPOCH 1\n",
      "Hospital 1 loss:tensor(0.3528)\n",
      "Hospital 2 loss:tensor(0.3537)\n",
      "LOCAL EPOCH 2\n",
      "Hospital 1 loss:tensor(0.2758)\n",
      "Hospital 2 loss:tensor(0.3039)\n",
      "LOCAL EPOCH 3\n",
      "Hospital 1 loss:tensor(0.2993)\n",
      "Hospital 2 loss:tensor(0.2630)\n",
      "LOCAL EPOCH 4\n",
      "Hospital 1 loss:tensor(0.3162)\n",
      "Hospital 2 loss:tensor(0.2964)\n",
      "Hospital1:tensor(0.2964) Hospital2:tensor(0.2964)\n",
      "------------------------------------\n",
      "GLOBAL EPOCH 3\n",
      "LOCAL EPOCH 0\n",
      "Hospital 1 loss:tensor(0.3129)\n",
      "Hospital 2 loss:tensor(0.2898)\n",
      "LOCAL EPOCH 1\n",
      "Hospital 1 loss:tensor(0.3279)\n",
      "Hospital 2 loss:tensor(0.2467)\n",
      "LOCAL EPOCH 2\n",
      "Hospital 1 loss:tensor(0.2739)\n",
      "Hospital 2 loss:tensor(0.2798)\n",
      "LOCAL EPOCH 3\n",
      "Hospital 1 loss:tensor(0.2555)\n",
      "Hospital 2 loss:tensor(0.2188)\n",
      "LOCAL EPOCH 4\n",
      "Hospital 1 loss:tensor(0.3276)\n",
      "Hospital 2 loss:tensor(0.2579)\n",
      "Hospital1:tensor(0.2579) Hospital2:tensor(0.2579)\n",
      "------------------------------------\n",
      "GLOBAL EPOCH 4\n",
      "LOCAL EPOCH 0\n",
      "Hospital 1 loss:tensor(0.2111)\n",
      "Hospital 2 loss:tensor(0.2244)\n",
      "LOCAL EPOCH 1\n",
      "Hospital 1 loss:tensor(0.2152)\n",
      "Hospital 2 loss:tensor(0.2047)\n",
      "LOCAL EPOCH 2\n",
      "Hospital 1 loss:tensor(0.1807)\n",
      "Hospital 2 loss:tensor(0.2199)\n",
      "LOCAL EPOCH 3\n",
      "Hospital 1 loss:tensor(0.1809)\n",
      "Hospital 2 loss:tensor(0.1850)\n",
      "LOCAL EPOCH 4\n",
      "Hospital 1 loss:tensor(0.1809)\n",
      "Hospital 2 loss:tensor(0.1740)\n",
      "Hospital1:tensor(0.1740) Hospital2:tensor(0.1740)\n",
      "------------------------------------\n",
      "GLOBAL EPOCH 5\n",
      "LOCAL EPOCH 0\n",
      "Hospital 1 loss:tensor(0.1630)\n",
      "Hospital 2 loss:tensor(0.2065)\n",
      "LOCAL EPOCH 1\n",
      "Hospital 1 loss:tensor(0.1616)\n",
      "Hospital 2 loss:tensor(0.1555)\n",
      "LOCAL EPOCH 2\n",
      "Hospital 1 loss:tensor(0.2046)\n",
      "Hospital 2 loss:tensor(0.1307)\n",
      "LOCAL EPOCH 3\n",
      "Hospital 1 loss:tensor(0.1499)\n",
      "Hospital 2 loss:tensor(0.1428)\n",
      "LOCAL EPOCH 4\n",
      "Hospital 1 loss:tensor(0.1316)\n",
      "Hospital 2 loss:tensor(0.1265)\n",
      "Hospital1:tensor(0.1265) Hospital2:tensor(0.1265)\n",
      "------------------------------------\n",
      "GLOBAL EPOCH 6\n",
      "LOCAL EPOCH 0\n",
      "Hospital 1 loss:tensor(0.1289)\n",
      "Hospital 2 loss:tensor(0.1071)\n",
      "LOCAL EPOCH 1\n",
      "Hospital 1 loss:tensor(0.0997)\n",
      "Hospital 2 loss:tensor(0.1159)\n",
      "LOCAL EPOCH 2\n",
      "Hospital 1 loss:tensor(0.0932)\n",
      "Hospital 2 loss:tensor(0.1475)\n",
      "LOCAL EPOCH 3\n",
      "Hospital 1 loss:tensor(0.0757)\n",
      "Hospital 2 loss:tensor(0.1077)\n",
      "LOCAL EPOCH 4\n",
      "Hospital 1 loss:tensor(0.0731)\n",
      "Hospital 2 loss:tensor(0.1115)\n",
      "Hospital1:tensor(0.1115) Hospital2:tensor(0.1115)\n",
      "------------------------------------\n",
      "GLOBAL EPOCH 7\n",
      "LOCAL EPOCH 0\n",
      "Hospital 1 loss:tensor(0.0807)\n",
      "Hospital 2 loss:tensor(0.1135)\n",
      "LOCAL EPOCH 1\n",
      "Hospital 1 loss:tensor(0.0798)\n",
      "Hospital 2 loss:tensor(0.0965)\n",
      "LOCAL EPOCH 2\n",
      "Hospital 1 loss:tensor(0.0559)\n",
      "Hospital 2 loss:tensor(0.0591)\n",
      "LOCAL EPOCH 3\n",
      "Hospital 1 loss:tensor(0.1030)\n",
      "Hospital 2 loss:tensor(0.0950)\n",
      "LOCAL EPOCH 4\n",
      "Hospital 1 loss:tensor(0.0680)\n",
      "Hospital 2 loss:tensor(0.0822)\n",
      "Hospital1:tensor(0.0822) Hospital2:tensor(0.0822)\n",
      "------------------------------------\n",
      "GLOBAL EPOCH 8\n",
      "LOCAL EPOCH 0\n",
      "Hospital 1 loss:tensor(0.0778)\n",
      "Hospital 2 loss:tensor(0.0615)\n",
      "LOCAL EPOCH 1\n",
      "Hospital 1 loss:tensor(0.0676)\n",
      "Hospital 2 loss:tensor(0.0665)\n",
      "LOCAL EPOCH 2\n",
      "Hospital 1 loss:tensor(0.0589)\n",
      "Hospital 2 loss:tensor(0.0563)\n",
      "LOCAL EPOCH 3\n",
      "Hospital 1 loss:tensor(0.0735)\n",
      "Hospital 2 loss:tensor(0.0516)\n",
      "LOCAL EPOCH 4\n",
      "Hospital 1 loss:tensor(0.0444)\n",
      "Hospital 2 loss:tensor(0.0483)\n",
      "Hospital1:tensor(0.0483) Hospital2:tensor(0.0483)\n",
      "------------------------------------\n",
      "GLOBAL EPOCH 9\n",
      "LOCAL EPOCH 0\n",
      "Hospital 1 loss:tensor(0.1060)\n",
      "Hospital 2 loss:tensor(0.0690)\n",
      "LOCAL EPOCH 1\n",
      "Hospital 1 loss:tensor(0.0784)\n",
      "Hospital 2 loss:tensor(0.0382)\n",
      "LOCAL EPOCH 2\n",
      "Hospital 1 loss:tensor(0.0456)\n",
      "Hospital 2 loss:tensor(0.0842)\n",
      "LOCAL EPOCH 3\n",
      "Hospital 1 loss:tensor(0.0511)\n",
      "Hospital 2 loss:tensor(0.0787)\n",
      "LOCAL EPOCH 4\n",
      "Hospital 1 loss:tensor(0.0448)\n",
      "Hospital 2 loss:tensor(0.0380)\n",
      "Hospital1:tensor(0.0380) Hospital2:tensor(0.0380)\n",
      "------------------------------------\n",
      "GLOBAL EPOCH 10\n",
      "LOCAL EPOCH 0\n",
      "Hospital 1 loss:tensor(0.0856)\n",
      "Hospital 2 loss:tensor(0.0972)\n",
      "LOCAL EPOCH 1\n",
      "Hospital 1 loss:tensor(0.0369)\n",
      "Hospital 2 loss:tensor(0.0508)\n",
      "LOCAL EPOCH 2\n",
      "Hospital 1 loss:tensor(0.0288)\n",
      "Hospital 2 loss:tensor(0.0638)\n",
      "LOCAL EPOCH 3\n",
      "Hospital 1 loss:tensor(0.0312)\n",
      "Hospital 2 loss:tensor(0.0571)\n",
      "LOCAL EPOCH 4\n",
      "Hospital 1 loss:tensor(0.0469)\n",
      "Hospital 2 loss:tensor(0.0518)\n",
      "Hospital1:tensor(0.0518) Hospital2:tensor(0.0518)\n",
      "------------------------------------\n",
      "GLOBAL EPOCH 11\n",
      "LOCAL EPOCH 0\n",
      "Hospital 1 loss:tensor(0.0491)\n",
      "Hospital 2 loss:tensor(0.0647)\n",
      "LOCAL EPOCH 1\n",
      "Hospital 1 loss:tensor(0.0928)\n",
      "Hospital 2 loss:tensor(0.0341)\n",
      "LOCAL EPOCH 2\n",
      "Hospital 1 loss:tensor(0.0452)\n",
      "Hospital 2 loss:tensor(0.0612)\n",
      "LOCAL EPOCH 3\n",
      "Hospital 1 loss:tensor(0.0456)\n",
      "Hospital 2 loss:tensor(0.0499)\n",
      "LOCAL EPOCH 4\n",
      "Hospital 1 loss:tensor(0.0673)\n",
      "Hospital 2 loss:tensor(0.0354)\n",
      "Hospital1:tensor(0.0354) Hospital2:tensor(0.0354)\n",
      "------------------------------------\n",
      "GLOBAL EPOCH 12\n",
      "LOCAL EPOCH 0\n",
      "Hospital 1 loss:tensor(0.0975)\n",
      "Hospital 2 loss:tensor(0.0391)\n",
      "LOCAL EPOCH 1\n",
      "Hospital 1 loss:tensor(0.0655)\n",
      "Hospital 2 loss:tensor(0.0576)\n",
      "LOCAL EPOCH 2\n",
      "Hospital 1 loss:tensor(0.0423)\n",
      "Hospital 2 loss:tensor(0.0501)\n",
      "LOCAL EPOCH 3\n",
      "Hospital 1 loss:tensor(0.0294)\n",
      "Hospital 2 loss:tensor(0.0347)\n",
      "LOCAL EPOCH 4\n",
      "Hospital 1 loss:tensor(0.0187)\n",
      "Hospital 2 loss:tensor(0.0459)\n",
      "Hospital1:tensor(0.0459) Hospital2:tensor(0.0459)\n",
      "------------------------------------\n",
      "GLOBAL EPOCH 13\n",
      "LOCAL EPOCH 0\n",
      "Hospital 1 loss:tensor(0.0616)\n",
      "Hospital 2 loss:tensor(0.0653)\n",
      "LOCAL EPOCH 1\n",
      "Hospital 1 loss:tensor(0.1090)\n",
      "Hospital 2 loss:tensor(0.0449)\n",
      "LOCAL EPOCH 2\n",
      "Hospital 1 loss:tensor(0.0198)\n",
      "Hospital 2 loss:tensor(0.0297)\n",
      "LOCAL EPOCH 3\n",
      "Hospital 1 loss:tensor(0.0534)\n",
      "Hospital 2 loss:tensor(0.0392)\n",
      "LOCAL EPOCH 4\n",
      "Hospital 1 loss:tensor(0.0308)\n",
      "Hospital 2 loss:tensor(0.0449)\n",
      "Hospital1:tensor(0.0449) Hospital2:tensor(0.0449)\n",
      "------------------------------------\n",
      "GLOBAL EPOCH 14\n",
      "LOCAL EPOCH 0\n",
      "Hospital 1 loss:tensor(0.0428)\n",
      "Hospital 2 loss:tensor(0.0471)\n",
      "LOCAL EPOCH 1\n",
      "Hospital 1 loss:tensor(0.0226)\n",
      "Hospital 2 loss:tensor(0.0309)\n",
      "LOCAL EPOCH 2\n",
      "Hospital 1 loss:tensor(0.0217)\n",
      "Hospital 2 loss:tensor(0.0687)\n",
      "LOCAL EPOCH 3\n",
      "Hospital 1 loss:tensor(0.0264)\n",
      "Hospital 2 loss:tensor(0.0533)\n",
      "LOCAL EPOCH 4\n",
      "Hospital 1 loss:tensor(0.0399)\n",
      "Hospital 2 loss:tensor(0.0355)\n",
      "Hospital1:tensor(0.0355) Hospital2:tensor(0.0355)\n",
      "------------------------------------\n",
      "GLOBAL EPOCH 15\n",
      "LOCAL EPOCH 0\n",
      "Hospital 1 loss:tensor(0.0686)\n",
      "Hospital 2 loss:tensor(0.0435)\n",
      "LOCAL EPOCH 1\n",
      "Hospital 1 loss:tensor(0.0283)\n",
      "Hospital 2 loss:tensor(0.0349)\n",
      "LOCAL EPOCH 2\n",
      "Hospital 1 loss:tensor(0.0540)\n",
      "Hospital 2 loss:tensor(0.0605)\n",
      "LOCAL EPOCH 3\n",
      "Hospital 1 loss:tensor(0.0363)\n",
      "Hospital 2 loss:tensor(0.0651)\n",
      "LOCAL EPOCH 4\n",
      "Hospital 1 loss:tensor(0.0364)\n",
      "Hospital 2 loss:tensor(0.0409)\n",
      "Hospital1:tensor(0.0409) Hospital2:tensor(0.0409)\n",
      "------------------------------------\n",
      "GLOBAL EPOCH 16\n",
      "LOCAL EPOCH 0\n",
      "Hospital 1 loss:tensor(0.0470)\n",
      "Hospital 2 loss:tensor(0.0355)\n",
      "LOCAL EPOCH 1\n",
      "Hospital 1 loss:tensor(0.0714)\n",
      "Hospital 2 loss:tensor(0.0389)\n",
      "LOCAL EPOCH 2\n",
      "Hospital 1 loss:tensor(0.0625)\n",
      "Hospital 2 loss:tensor(0.0415)\n",
      "LOCAL EPOCH 3\n",
      "Hospital 1 loss:tensor(0.0117)\n",
      "Hospital 2 loss:tensor(0.0294)\n",
      "LOCAL EPOCH 4\n",
      "Hospital 1 loss:tensor(0.0363)\n",
      "Hospital 2 loss:tensor(0.0391)\n",
      "Hospital1:tensor(0.0391) Hospital2:tensor(0.0391)\n",
      "------------------------------------\n",
      "GLOBAL EPOCH 17\n",
      "LOCAL EPOCH 0\n",
      "Hospital 1 loss:tensor(0.0802)\n",
      "Hospital 2 loss:tensor(0.0570)\n",
      "LOCAL EPOCH 1\n",
      "Hospital 1 loss:tensor(0.0309)\n",
      "Hospital 2 loss:tensor(0.0288)\n",
      "LOCAL EPOCH 2\n",
      "Hospital 1 loss:tensor(0.0204)\n",
      "Hospital 2 loss:tensor(0.0702)\n",
      "LOCAL EPOCH 3\n",
      "Hospital 1 loss:tensor(0.0521)\n",
      "Hospital 2 loss:tensor(0.0376)\n",
      "LOCAL EPOCH 4\n",
      "Hospital 1 loss:tensor(0.0522)\n",
      "Hospital 2 loss:tensor(0.0387)\n",
      "Hospital1:tensor(0.0387) Hospital2:tensor(0.0387)\n",
      "------------------------------------\n",
      "GLOBAL EPOCH 18\n",
      "LOCAL EPOCH 0\n",
      "Hospital 1 loss:tensor(0.0194)\n",
      "Hospital 2 loss:tensor(0.0391)\n",
      "LOCAL EPOCH 1\n",
      "Hospital 1 loss:tensor(0.0260)\n",
      "Hospital 2 loss:tensor(0.0421)\n",
      "LOCAL EPOCH 2\n",
      "Hospital 1 loss:tensor(0.0374)\n",
      "Hospital 2 loss:tensor(0.0708)\n",
      "LOCAL EPOCH 3\n",
      "Hospital 1 loss:tensor(0.0231)\n",
      "Hospital 2 loss:tensor(0.0397)\n",
      "LOCAL EPOCH 4\n",
      "Hospital 1 loss:tensor(0.0286)\n",
      "Hospital 2 loss:tensor(0.0583)\n",
      "Hospital1:tensor(0.0583) Hospital2:tensor(0.0583)\n",
      "------------------------------------\n",
      "GLOBAL EPOCH 19\n",
      "LOCAL EPOCH 0\n",
      "Hospital 1 loss:tensor(0.0353)\n",
      "Hospital 2 loss:tensor(0.0516)\n",
      "LOCAL EPOCH 1\n",
      "Hospital 1 loss:tensor(0.0337)\n",
      "Hospital 2 loss:tensor(0.0363)\n",
      "LOCAL EPOCH 2\n",
      "Hospital 1 loss:tensor(0.0252)\n",
      "Hospital 2 loss:tensor(0.0401)\n",
      "LOCAL EPOCH 3\n",
      "Hospital 1 loss:tensor(0.0220)\n",
      "Hospital 2 loss:tensor(0.0557)\n",
      "LOCAL EPOCH 4\n",
      "Hospital 1 loss:tensor(0.0100)\n",
      "Hospital 2 loss:tensor(0.0151)\n",
      "Hospital1:tensor(0.0151) Hospital2:tensor(0.0151)\n"
     ]
    }
   ],
   "source": [
    "# epoch vars for local and global iterations\n",
    "GLOBAL_EPOCHS = 20\n",
    "LOCAL_EPOCHS = 5\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "for e in range(GLOBAL_EPOCHS):\n",
    "    print('------------------------------------\\nGLOBAL EPOCH %d' % e)\n",
    "    # send the current version of the model to the hospitals\n",
    "    hosp1_model = model.copy().send(hosp1)\n",
    "    hosp2_model = model.copy().send(hosp2)\n",
    "    \n",
    "    # redefine the optimizer\n",
    "    hosp1_opt = optim.SGD(params=hosp1_model.parameters(),lr=5e-3, momentum=0.9)\n",
    "    hosp2_opt = optim.SGD(params=hosp2_model.parameters(),lr=5e-3, momentum=0.9)\n",
    "\n",
    "    for le in range(LOCAL_EPOCHS):\n",
    "        print('LOCAL EPOCH %d' % le)\n",
    "        # train hospital1's model\n",
    "        batch_num = int(np.ceil(hosp1_data_n / BATCH_SIZE))\n",
    "        for b in range(batch_num):\n",
    "            ix = get_ix(b, BATCH_SIZE, hosp1_data_n)\n",
    "            b_data = hosp1_data[ix]\n",
    "            b_target = hosp1_target[ix]\n",
    "            hosp1_opt.zero_grad()\n",
    "            hosp1_pred = hosp1_model(b_data)\n",
    "            hosp1_loss = criterion(hosp1_pred, b_target)\n",
    "            hosp1_loss.backward()\n",
    "            hosp1_opt.step()\n",
    "        \n",
    "        # shuffle hospital data and target\n",
    "        si = torch.randperm(hosp1_data_n).send(hosp1)\n",
    "        hosp1_data = hosp1_data[si]\n",
    "        hosp1_target = hosp1_target[si]\n",
    "        # print progress\n",
    "        hosp1_loss = hosp1_loss.get().data\n",
    "        print('Hospital 1 loss:' + str(hosp1_loss))\n",
    "\n",
    "        # train hospital2's model\n",
    "        batch_num = int(np.ceil(hosp2_data_n / BATCH_SIZE))\n",
    "        for b in range(batch_num):\n",
    "            ix = get_ix(b, BATCH_SIZE, hosp2_data_n)\n",
    "            b_data = hosp2_data[ix]\n",
    "            b_target = hosp2_target[ix]\n",
    "            hosp2_opt.zero_grad()\n",
    "            hosp2_pred = hosp2_model(b_data)\n",
    "            hosp2_loss = criterion(hosp2_pred, b_target)\n",
    "            hosp2_loss.backward()\n",
    "            hosp2_opt.step()\n",
    "        \n",
    "        # shuffle hospital data and target\n",
    "        si = torch.randperm(hosp2_data_n).send(hosp2)\n",
    "        hosp2_data = hosp2_data[si]\n",
    "        hosp2_target = hosp2_target[si]\n",
    "        # print progress\n",
    "        hosp2_loss = hosp2_loss.get().data\n",
    "        print('Hospital 2 loss:' + str(hosp2_loss))\n",
    "              \n",
    "    # Now that each data owner has a partially trained model, it's time to average them\n",
    "    # together in a secure way. We achieve this by instructing Hospital 1 and 2 to send \n",
    "    # their model to the secure (trusted) server. \n",
    "    hosp1_model.move(secure_worker)\n",
    "    hosp2_model.move(secure_worker)\n",
    "    \n",
    "    # Finally, the last step for this training epoch is to average the two trained models \n",
    "    # together and then use this to set the values for our global \"model\". \n",
    "    with torch.no_grad():\n",
    "        all_params = zip(\n",
    "            model.parameters(),\n",
    "            hosp1_model.parameters(), \n",
    "            hosp2_model.parameters()\n",
    "        )\n",
    "        for p0, p1, p2 in all_params:\n",
    "            p0.set_(((p1 + p2) / 2).get())\n",
    "    \n",
    "    # report on progress\n",
    "    print(\"Hospital1:\" + str(hosp2_loss) + \" Hospital2:\" + str(hosp2_loss))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we want to make sure that our resulting model learned correctly, so we'll evaluate it on a test dataset.  In this toy problem, we'll use the original data, but in practice we'll want to use new data to understand how well the model generalizes to unseen examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(torch.tensor(X_test))\n",
    "preds = preds.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prec, rec, _ = precision_recall_curve(y_test, preds)\n",
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plt.figure()\n",
    "plt.plot(rec, prec, color='darkorange', label='Fed-DL')\n",
    "plt.xlim([0.0, 1.05])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
