{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated MNIST multiclass classification using ConvNet\n",
    "\n",
    "- Separate MNIST dataset by digits. \n",
    "- First server has 0-3, second 4-6, third 7-9\n",
    "- By the end of federated training with averaging they all know how to correctly classify all 10 digits in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "from neoglia.workers.connect_workers import connect\n",
    "from neoglia.learn.utils import setup_logging\n",
    "from neoglia.learn.config import LearnConfig\n",
    "from neoglia.learn.losses import cross_entropy\n",
    "from neoglia.learn.models import ConvNet\n",
    "from neoglia.learn.learner import Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler(sys.stderr)\n",
    "formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.handlers = [handler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to data nodes\n",
    "\n",
    "In this demo, we have 3 distinct hospitals. Each is an indenpendent EC2 instance on AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neoglia.workers.connect_workers - INFO - Connected to worker h1.\n",
      "neoglia.workers.connect_workers - INFO - Connected to worker h2.\n",
      "neoglia.workers.connect_workers - INFO - Connected to worker h3.\n"
     ]
    }
   ],
   "source": [
    "h1, h2, h3 = connect(local=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "root - INFO - -mnist_train:\n",
      "\tdata size: [24754, 28, 28],\n",
      "\ttarget size: [24754]\n",
      "-mnist_test:\n",
      "\tdata size: [10000, 28, 28],\n",
      "\ttarget size: [10000]\n",
      "-eicu_class_train:\n",
      "\tdata size: [4777, 103],\n",
      "\ttarget size: [4777]\n",
      "-eicu_class_test:\n",
      "\tdata size: [5389, 103],\n",
      "\ttarget size: [5389]\n",
      "-eicu_reg_train:\n",
      "\tdata size: [4777, 103],\n",
      "\ttarget size: [4777]\n",
      "-eicu_reg_test:\n",
      "\tdata size: [5389, 103],\n",
      "\ttarget size: [5389]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(h1.list_datasets())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the datasets they have and the dimensions of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a convolutional neural network on the mnist dataset with federated averaging\n",
    "\n",
    "Each hospital holds a subset of the training data but they all share the same test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the config file for this experiment\n",
    "\n",
    "This holds everything from the learning rate to the batch size. \n",
    "\n",
    "First let's check the available parameters. Note, this object can take a yml config file (good for reproducible experiments) or be parametrised when instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mLearnConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconfig_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrain_dataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtest_dataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtest_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrain_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfed_after_n_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msave_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      Config dict object, holding all parameters for the training and evaluation.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Constructor of the subclassed dict object.\n",
       "\n",
       "Args:\n",
       "    config_file (str): Location of config YAML file. If provided, all\n",
       "        parameters that are defined within will override the defaults here.\n",
       "    train_dataset_name (str): Name of the remote dataset to train on.\n",
       "    test_dataset_name (str): Name of the remote dataset to test on.\n",
       "    train_batch_size (int): Batch size for training.\n",
       "    test_batch_size (int): Batch size for evaluation.\n",
       "    train_epochs (int): Number of epochs performed altogether for training on\n",
       "        remote workers.\n",
       "    fed_after_n_batches (int): Number of training epochs performed on each\n",
       "        remote worker before averaging global model.\n",
       "    metrics (tuple<str>): Metrics to use for evaluation of the model. Use any\n",
       "        of: accuracy, precision, recall, mse, mae.\n",
       "    lr (float): Learning rate for the optimizer.\n",
       "    cuda (bool): Whether the remote workers have GPUs and CUDA enabled.\n",
       "    seed (int): Seed for reproducibility.\n",
       "    save_model (bool): Whether to save the global model. If yes, it is\n",
       "        saved where the python interpreter is running.\n",
       "    verbose (bool): Verbosity - false: not entirely silent, but quite minimal.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/Dropbox/NG/proto/proto/src/neoglia/learn/config.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?LearnConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config_file': 'config_mnist.yml',\n",
       " 'train_dataset_name': 'mnist_train',\n",
       " 'test_dataset_name': 'mnist_test',\n",
       " 'train_batch_size': 128,\n",
       " 'test_batch_size': 128,\n",
       " 'train_epochs': 50,\n",
       " 'fed_after_n_batches': 10,\n",
       " 'metrics': ['accuracy'],\n",
       " 'optimizer': 'SGD',\n",
       " 'optimizer_params': {'lr': 0.1, 'momentum': 0.9},\n",
       " 'cuda': False,\n",
       " 'seed': 42,\n",
       " 'save_model': True,\n",
       " 'verbose': True,\n",
       " 'regression': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = LearnConfig(\"config_mnist.yml\")\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model architecture and loss function\n",
    "\n",
    "Define a model architecture in Torch, or simply load one of NeoGlia's predefined ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;32mclass\u001b[0m \u001b[0mConvNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    Simple convolutional neural network for multi-class image data.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Returns probabilities after softmax and not logits.\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConvNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%psource ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use cross entropy in this example as a loss function as this is a multi-class problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training and evaluating the model in a federated manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_learner = Learner(\n",
    "    config=config,\n",
    "    model=model, \n",
    "    model_input_dim=[1, 1, 28, 28],\n",
    "    loss_fn=cross_entropy, \n",
    "    workers=(h1, h2, h3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neoglia.learn.learner - INFO - Starting epoch 1/50\n",
      "neoglia.learn.learner - INFO - Training round: 1, worker: h2, avg_loss: 1.4678\n",
      "neoglia.learn.learner - INFO - Training round: 1, worker: h3, avg_loss: 1.2387\n",
      "neoglia.learn.learner - INFO - Training round: 1, worker: h1, avg_loss: 1.7557\n",
      "neoglia.learn.learner - INFO - Starting epoch 2/50\n",
      "neoglia.learn.learner - INFO - Training round: 2, worker: h2, avg_loss: 1.6916\n",
      "neoglia.learn.learner - INFO - Training round: 2, worker: h3, avg_loss: 0.8026\n",
      "neoglia.learn.learner - INFO - Training round: 2, worker: h1, avg_loss: 1.9385\n",
      "neoglia.learn.learner - INFO - Starting epoch 3/50\n",
      "neoglia.learn.learner - INFO - Training round: 3, worker: h2, avg_loss: 0.4517\n",
      "neoglia.learn.learner - INFO - Training round: 3, worker: h1, avg_loss: 0.9103\n",
      "neoglia.learn.learner - INFO - Training round: 3, worker: h3, avg_loss: 0.9021\n",
      "neoglia.learn.learner - INFO - Starting epoch 4/50\n",
      "neoglia.learn.learner - INFO - Training round: 4, worker: h1, avg_loss: 0.4707\n",
      "neoglia.learn.learner - INFO - Training round: 4, worker: h3, avg_loss: 0.3962\n",
      "neoglia.learn.learner - INFO - Training round: 4, worker: h2, avg_loss: 0.8216\n",
      "neoglia.learn.learner - INFO - Starting epoch 5/50\n",
      "neoglia.learn.learner - INFO - Training round: 5, worker: h1, avg_loss: 1.0356\n",
      "neoglia.learn.learner - INFO - Training round: 5, worker: h3, avg_loss: 0.1718\n",
      "neoglia.learn.learner - INFO - Training round: 5, worker: h2, avg_loss: 0.4633\n",
      "neoglia.learn.learner - INFO - Starting epoch 6/50\n",
      "neoglia.learn.learner - INFO - Training round: 6, worker: h1, avg_loss: 0.3807\n",
      "neoglia.learn.learner - INFO - Training round: 6, worker: h3, avg_loss: 0.3899\n",
      "neoglia.learn.learner - INFO - Training round: 6, worker: h2, avg_loss: 0.0971\n",
      "neoglia.learn.learner - INFO - Starting epoch 7/50\n",
      "neoglia.learn.learner - INFO - Training round: 7, worker: h3, avg_loss: 1.3706\n",
      "neoglia.learn.learner - INFO - Training round: 7, worker: h1, avg_loss: 0.3672\n",
      "neoglia.learn.learner - INFO - Training round: 7, worker: h2, avg_loss: 0.4198\n",
      "neoglia.learn.learner - INFO - Starting epoch 8/50\n",
      "neoglia.learn.learner - INFO - Training round: 8, worker: h2, avg_loss: 0.2667\n",
      "neoglia.learn.learner - INFO - Training round: 8, worker: h3, avg_loss: 0.1651\n",
      "neoglia.learn.learner - INFO - Training round: 8, worker: h1, avg_loss: 0.2672\n",
      "neoglia.learn.learner - INFO - Starting epoch 9/50\n",
      "neoglia.learn.learner - INFO - Training round: 9, worker: h2, avg_loss: 0.1409\n",
      "neoglia.learn.learner - INFO - Training round: 9, worker: h1, avg_loss: 0.1467\n",
      "neoglia.learn.learner - INFO - Training round: 9, worker: h3, avg_loss: 0.2456\n",
      "neoglia.learn.learner - INFO - Starting epoch 10/50\n",
      "neoglia.learn.learner - INFO - Training round: 10, worker: h2, avg_loss: 0.5906\n",
      "neoglia.learn.learner - INFO - Training round: 10, worker: h3, avg_loss: 0.2540\n",
      "neoglia.learn.learner - INFO - Training round: 10, worker: h1, avg_loss: 0.1102\n",
      "neoglia.learn.learner - INFO - h1: Test set: Average loss: 0.0556\n",
      "neoglia.learn.learner - INFO - \t- accuracy: 0.2101\n",
      "neoglia.learn.learner - INFO - h2: Test set: Average loss: 0.0238\n",
      "neoglia.learn.learner - INFO - \t- accuracy: 0.1135\n",
      "neoglia.learn.learner - INFO - h3: Test set: Average loss: 0.0377\n",
      "neoglia.learn.learner - INFO - \t- accuracy: 0.1149\n",
      "neoglia.learn.learner - INFO - Federated model: Test set: Average loss: 0.0064\n",
      "neoglia.learn.learner - INFO - \t- accuracy: 0.1979\n",
      "neoglia.learn.learner - INFO - Starting epoch 11/50\n",
      "neoglia.learn.learner - INFO - Training round: 11, worker: h2, avg_loss: 0.0385\n",
      "neoglia.learn.learner - INFO - Training round: 11, worker: h3, avg_loss: 0.2156\n",
      "neoglia.learn.learner - INFO - Training round: 11, worker: h1, avg_loss: 0.1093\n",
      "neoglia.learn.learner - INFO - Starting epoch 12/50\n",
      "neoglia.learn.learner - INFO - Training round: 12, worker: h2, avg_loss: 0.2839\n",
      "neoglia.learn.learner - INFO - Training round: 12, worker: h1, avg_loss: 0.2114\n",
      "neoglia.learn.learner - INFO - Training round: 12, worker: h3, avg_loss: 0.0641\n",
      "neoglia.learn.learner - INFO - Starting epoch 13/50\n",
      "neoglia.learn.learner - INFO - Training round: 13, worker: h2, avg_loss: 0.0686\n",
      "neoglia.learn.learner - INFO - Training round: 13, worker: h3, avg_loss: 0.2468\n",
      "neoglia.learn.learner - INFO - Training round: 13, worker: h1, avg_loss: 0.2381\n",
      "neoglia.learn.learner - INFO - Starting epoch 14/50\n",
      "neoglia.learn.learner - INFO - Training round: 14, worker: h2, avg_loss: 0.0110\n",
      "neoglia.learn.learner - INFO - Training round: 14, worker: h1, avg_loss: 0.0698\n",
      "neoglia.learn.learner - INFO - Training round: 14, worker: h3, avg_loss: 0.1874\n",
      "neoglia.learn.learner - INFO - Starting epoch 15/50\n",
      "neoglia.learn.learner - INFO - Training round: 15, worker: h3, avg_loss: 0.1585\n",
      "neoglia.learn.learner - INFO - Training round: 15, worker: h1, avg_loss: 0.0804\n",
      "neoglia.learn.learner - INFO - Training round: 15, worker: h2, avg_loss: 0.0923\n",
      "neoglia.learn.learner - INFO - Starting epoch 16/50\n",
      "neoglia.learn.learner - INFO - Training round: 16, worker: h1, avg_loss: 0.0455\n",
      "neoglia.learn.learner - INFO - Training round: 16, worker: h3, avg_loss: 0.0666\n",
      "neoglia.learn.learner - INFO - Training round: 16, worker: h2, avg_loss: 0.1444\n",
      "neoglia.learn.learner - INFO - Starting epoch 17/50\n",
      "neoglia.learn.learner - INFO - Training round: 17, worker: h2, avg_loss: 0.0402\n",
      "neoglia.learn.learner - INFO - Training round: 17, worker: h3, avg_loss: 0.1791\n",
      "neoglia.learn.learner - INFO - Training round: 17, worker: h1, avg_loss: 0.1364\n",
      "neoglia.learn.learner - INFO - Starting epoch 18/50\n",
      "neoglia.learn.learner - INFO - Training round: 18, worker: h2, avg_loss: 0.0842\n",
      "neoglia.learn.learner - INFO - Training round: 18, worker: h3, avg_loss: 0.2246\n",
      "neoglia.learn.learner - INFO - Training round: 18, worker: h1, avg_loss: 0.0284\n",
      "neoglia.learn.learner - INFO - Starting epoch 19/50\n",
      "neoglia.learn.learner - INFO - Training round: 19, worker: h3, avg_loss: 0.1323\n",
      "neoglia.learn.learner - INFO - Training round: 19, worker: h1, avg_loss: 0.0521\n",
      "neoglia.learn.learner - INFO - Training round: 19, worker: h2, avg_loss: 0.2053\n",
      "neoglia.learn.learner - INFO - Starting epoch 20/50\n",
      "neoglia.learn.learner - INFO - Training round: 20, worker: h2, avg_loss: 0.0043\n",
      "neoglia.learn.learner - INFO - Training round: 20, worker: h1, avg_loss: 0.1357\n",
      "neoglia.learn.learner - INFO - Training round: 20, worker: h3, avg_loss: 0.3031\n",
      "neoglia.learn.learner - INFO - h1: Test set: Average loss: 0.0146\n",
      "neoglia.learn.learner - INFO - \t- accuracy: 0.2097\n",
      "neoglia.learn.learner - INFO - h2: Test set: Average loss: 0.0234\n",
      "neoglia.learn.learner - INFO - \t- accuracy: 0.1230\n",
      "neoglia.learn.learner - INFO - h3: Test set: Average loss: 0.0386\n",
      "neoglia.learn.learner - INFO - \t- accuracy: 0.1194\n",
      "neoglia.learn.learner - INFO - Federated model: Test set: Average loss: 0.0028\n",
      "neoglia.learn.learner - INFO - \t- accuracy: 0.2002\n",
      "neoglia.learn.learner - INFO - Starting epoch 21/50\n",
      "neoglia.learn.learner - INFO - Training round: 21, worker: h2, avg_loss: 0.0607\n",
      "neoglia.learn.learner - INFO - Training round: 21, worker: h3, avg_loss: 0.0365\n",
      "neoglia.learn.learner - INFO - Training round: 21, worker: h1, avg_loss: 0.2711\n",
      "neoglia.learn.learner - INFO - Starting epoch 22/50\n",
      "neoglia.learn.learner - INFO - Training round: 22, worker: h2, avg_loss: 0.0975\n",
      "neoglia.learn.learner - INFO - Training round: 22, worker: h3, avg_loss: 0.0759\n",
      "neoglia.learn.learner - INFO - Training round: 22, worker: h1, avg_loss: 0.1077\n",
      "neoglia.learn.learner - INFO - Starting epoch 23/50\n",
      "neoglia.learn.learner - INFO - Training round: 23, worker: h2, avg_loss: 0.0160\n",
      "neoglia.learn.learner - INFO - Training round: 23, worker: h3, avg_loss: 0.0182\n",
      "neoglia.learn.learner - INFO - Training round: 23, worker: h1, avg_loss: 0.0660\n",
      "neoglia.learn.learner - INFO - Starting epoch 24/50\n",
      "neoglia.learn.learner - INFO - Training round: 24, worker: h3, avg_loss: 0.1702\n",
      "neoglia.learn.learner - INFO - Training round: 24, worker: h2, avg_loss: 0.0371\n",
      "neoglia.learn.learner - INFO - Training round: 24, worker: h1, avg_loss: 0.0587\n",
      "neoglia.learn.learner - INFO - Starting epoch 25/50\n",
      "neoglia.learn.learner - INFO - Training round: 25, worker: h3, avg_loss: 0.3885\n",
      "neoglia.learn.learner - INFO - Training round: 25, worker: h2, avg_loss: 0.0493\n",
      "neoglia.learn.learner - INFO - Training round: 25, worker: h1, avg_loss: 0.0492\n",
      "neoglia.learn.learner - INFO - Starting epoch 26/50\n",
      "neoglia.learn.learner - INFO - Training round: 26, worker: h3, avg_loss: 0.0681\n",
      "neoglia.learn.learner - INFO - Training round: 26, worker: h1, avg_loss: 0.1349\n",
      "neoglia.learn.learner - INFO - Training round: 26, worker: h2, avg_loss: 0.0189\n",
      "neoglia.learn.learner - INFO - Starting epoch 27/50\n",
      "neoglia.learn.learner - INFO - Training round: 27, worker: h2, avg_loss: 0.0320\n",
      "neoglia.learn.learner - INFO - Training round: 27, worker: h3, avg_loss: 0.1640\n",
      "neoglia.learn.learner - INFO - Training round: 27, worker: h1, avg_loss: 0.0383\n",
      "neoglia.learn.learner - INFO - Starting epoch 28/50\n",
      "neoglia.learn.learner - INFO - Training round: 28, worker: h2, avg_loss: 0.0032\n",
      "neoglia.learn.learner - INFO - Training round: 28, worker: h3, avg_loss: 0.1062\n",
      "neoglia.learn.learner - INFO - Training round: 28, worker: h1, avg_loss: 0.0879\n",
      "neoglia.learn.learner - INFO - Starting epoch 29/50\n",
      "neoglia.learn.learner - INFO - Training round: 29, worker: h2, avg_loss: 0.0438\n",
      "neoglia.learn.learner - INFO - Training round: 29, worker: h3, avg_loss: 0.0804\n",
      "neoglia.learn.learner - INFO - Training round: 29, worker: h1, avg_loss: 0.0186\n",
      "neoglia.learn.learner - INFO - Starting epoch 30/50\n",
      "neoglia.learn.learner - INFO - Training round: 30, worker: h2, avg_loss: 0.0056\n",
      "neoglia.learn.learner - INFO - Training round: 30, worker: h3, avg_loss: 0.1094\n",
      "neoglia.learn.learner - INFO - Training round: 30, worker: h1, avg_loss: 0.0568\n",
      "neoglia.learn.learner - INFO - h1: Test set: Average loss: 0.0136\n",
      "neoglia.learn.learner - INFO - \t- accuracy: 0.2102\n",
      "neoglia.learn.learner - INFO - h2: Test set: Average loss: 0.0083\n",
      "neoglia.learn.learner - INFO - \t- accuracy: 0.1749\n",
      "neoglia.learn.learner - INFO - h3: Test set: Average loss: 0.0098\n",
      "neoglia.learn.learner - INFO - \t- accuracy: 0.1681\n",
      "neoglia.learn.learner - INFO - Federated model: Test set: Average loss: 0.0012\n",
      "neoglia.learn.learner - INFO - \t- accuracy: 0.2096\n",
      "neoglia.learn.learner - INFO - Starting epoch 31/50\n",
      "neoglia.learn.learner - INFO - Training round: 31, worker: h2, avg_loss: 0.0046\n",
      "neoglia.learn.learner - INFO - Training round: 31, worker: h3, avg_loss: 0.0596\n",
      "neoglia.learn.learner - INFO - Training round: 31, worker: h1, avg_loss: 0.0157\n",
      "neoglia.learn.learner - INFO - Starting epoch 32/50\n",
      "neoglia.learn.learner - INFO - Training round: 32, worker: h2, avg_loss: 0.0299\n",
      "neoglia.learn.learner - INFO - Training round: 32, worker: h1, avg_loss: 0.0420\n",
      "neoglia.learn.learner - INFO - Training round: 32, worker: h3, avg_loss: 0.0388\n",
      "neoglia.learn.learner - INFO - Starting epoch 33/50\n",
      "neoglia.learn.learner - INFO - Training round: 33, worker: h2, avg_loss: 0.0041\n",
      "neoglia.learn.learner - INFO - Training round: 33, worker: h3, avg_loss: 0.0287\n",
      "neoglia.learn.learner - INFO - Training round: 33, worker: h1, avg_loss: 0.1071\n",
      "neoglia.learn.learner - INFO - Starting epoch 34/50\n",
      "neoglia.learn.learner - INFO - Training round: 34, worker: h2, avg_loss: 0.1096\n",
      "neoglia.learn.learner - INFO - Training round: 34, worker: h3, avg_loss: 0.0116\n",
      "neoglia.learn.learner - INFO - Training round: 34, worker: h1, avg_loss: 0.0057\n",
      "neoglia.learn.learner - INFO - Starting epoch 35/50\n",
      "neoglia.learn.learner - INFO - Training round: 35, worker: h2, avg_loss: 0.0301\n",
      "neoglia.learn.learner - INFO - Training round: 35, worker: h3, avg_loss: 0.0564\n",
      "neoglia.learn.learner - INFO - Training round: 35, worker: h1, avg_loss: 0.0077\n",
      "neoglia.learn.learner - INFO - Starting epoch 36/50\n",
      "neoglia.learn.learner - INFO - Training round: 36, worker: h2, avg_loss: 0.0312\n",
      "neoglia.learn.learner - INFO - Training round: 36, worker: h3, avg_loss: 0.0725\n",
      "neoglia.learn.learner - INFO - Training round: 36, worker: h1, avg_loss: 0.0148\n",
      "neoglia.learn.learner - INFO - Starting epoch 37/50\n",
      "neoglia.learn.learner - INFO - Training round: 37, worker: h2, avg_loss: 0.0316\n",
      "neoglia.learn.learner - INFO - Training round: 37, worker: h3, avg_loss: 0.0810\n",
      "neoglia.learn.learner - INFO - Training round: 37, worker: h1, avg_loss: 0.1078\n",
      "neoglia.learn.learner - INFO - Starting epoch 38/50\n",
      "neoglia.learn.learner - INFO - Training round: 38, worker: h2, avg_loss: 0.0008\n",
      "neoglia.learn.learner - INFO - Training round: 38, worker: h3, avg_loss: 0.0311\n",
      "neoglia.learn.learner - INFO - Training round: 38, worker: h1, avg_loss: 0.0035\n",
      "neoglia.learn.learner - INFO - Starting epoch 39/50\n",
      "neoglia.learn.learner - INFO - Training round: 39, worker: h2, avg_loss: 0.0025\n",
      "neoglia.learn.learner - INFO - Training round: 39, worker: h3, avg_loss: 0.0098\n",
      "neoglia.learn.learner - INFO - Training round: 39, worker: h1, avg_loss: 0.0295\n",
      "neoglia.learn.learner - INFO - Starting epoch 40/50\n",
      "neoglia.learn.learner - INFO - Training round: 40, worker: h3, avg_loss: 0.0307\n",
      "neoglia.learn.learner - INFO - Training round: 40, worker: h1, avg_loss: 0.0471\n",
      "neoglia.learn.learner - INFO - Training round: 40, worker: h2, avg_loss: 0.0032\n",
      "neoglia.learn.learner - INFO - h1: Test set: Average loss: 0.0129\n",
      "neoglia.learn.learner - INFO - \t- accuracy: 0.2108\n",
      "neoglia.learn.learner - INFO - h2: Test set: Average loss: 0.0122\n",
      "neoglia.learn.learner - INFO - \t- accuracy: 0.1349\n",
      "neoglia.learn.learner - INFO - h3: Test set: Average loss: 0.0127\n",
      "neoglia.learn.learner - INFO - \t- accuracy: 0.1754\n",
      "neoglia.learn.learner - INFO - Federated model: Test set: Average loss: 0.0008\n",
      "neoglia.learn.learner - INFO - \t- accuracy: 0.2082\n",
      "neoglia.learn.learner - INFO - Starting epoch 41/50\n",
      "neoglia.learn.learner - INFO - Training round: 41, worker: h2, avg_loss: 0.0148\n",
      "neoglia.learn.learner - INFO - Training round: 41, worker: h3, avg_loss: 0.1629\n",
      "neoglia.learn.learner - INFO - Training round: 41, worker: h1, avg_loss: 0.0506\n",
      "neoglia.learn.learner - INFO - Starting epoch 42/50\n",
      "neoglia.learn.learner - INFO - Training round: 42, worker: h2, avg_loss: 0.0027\n",
      "neoglia.learn.learner - INFO - Training round: 42, worker: h3, avg_loss: 0.0313\n",
      "neoglia.learn.learner - INFO - Training round: 42, worker: h1, avg_loss: 0.0253\n",
      "neoglia.learn.learner - INFO - Starting epoch 43/50\n",
      "neoglia.learn.learner - INFO - Training round: 43, worker: h2, avg_loss: 0.0066\n",
      "neoglia.learn.learner - INFO - Training round: 43, worker: h1, avg_loss: 0.0020\n",
      "neoglia.learn.learner - INFO - Training round: 43, worker: h3, avg_loss: 0.0763\n",
      "neoglia.learn.learner - INFO - Starting epoch 44/50\n",
      "neoglia.learn.learner - INFO - Training round: 44, worker: h2, avg_loss: 0.0032\n",
      "neoglia.learn.learner - INFO - Training round: 44, worker: h3, avg_loss: 0.0573\n",
      "neoglia.learn.learner - INFO - Training round: 44, worker: h1, avg_loss: 0.0208\n",
      "neoglia.learn.learner - INFO - Starting epoch 45/50\n",
      "neoglia.learn.learner - INFO - Training round: 45, worker: h2, avg_loss: 0.0250\n",
      "neoglia.learn.learner - INFO - Training round: 45, worker: h1, avg_loss: 0.0185\n",
      "neoglia.learn.learner - INFO - Training round: 45, worker: h3, avg_loss: 0.1461\n",
      "neoglia.learn.learner - INFO - Starting epoch 46/50\n",
      "neoglia.learn.learner - INFO - Training round: 46, worker: h1, avg_loss: 0.0187\n",
      "neoglia.learn.learner - INFO - Training round: 46, worker: h2, avg_loss: 0.0030\n",
      "neoglia.learn.learner - INFO - Training round: 46, worker: h3, avg_loss: 0.0219\n",
      "neoglia.learn.learner - INFO - Starting epoch 47/50\n",
      "neoglia.learn.learner - INFO - Training round: 47, worker: h2, avg_loss: 0.0240\n",
      "neoglia.learn.learner - INFO - Training round: 47, worker: h3, avg_loss: 0.0163\n",
      "neoglia.learn.learner - INFO - Training round: 47, worker: h1, avg_loss: 0.1076\n",
      "neoglia.learn.learner - INFO - Starting epoch 48/50\n",
      "neoglia.learn.learner - INFO - Training round: 48, worker: h1, avg_loss: 0.0328\n",
      "neoglia.learn.learner - INFO - Training round: 48, worker: h3, avg_loss: 0.0072\n",
      "neoglia.learn.learner - INFO - Training round: 48, worker: h2, avg_loss: 0.0266\n",
      "neoglia.learn.learner - INFO - Starting epoch 49/50\n",
      "neoglia.learn.learner - INFO - Training round: 49, worker: h2, avg_loss: 0.0136\n",
      "neoglia.learn.learner - INFO - Training round: 49, worker: h3, avg_loss: 0.0527\n",
      "neoglia.learn.learner - INFO - Training round: 49, worker: h1, avg_loss: 0.0351\n",
      "neoglia.learn.learner - INFO - Starting epoch 50/50\n",
      "neoglia.learn.learner - INFO - Training round: 50, worker: h2, avg_loss: 0.0130\n",
      "neoglia.learn.learner - INFO - Training round: 50, worker: h3, avg_loss: 0.0338\n",
      "neoglia.learn.learner - INFO - Training round: 50, worker: h1, avg_loss: 0.0344\n",
      "neoglia.learn.learner - INFO - h1: Test set: Average loss: 0.0096\n",
      "neoglia.learn.learner - INFO - \t- accuracy: 0.2114\n",
      "neoglia.learn.learner - INFO - h2: Test set: Average loss: 0.0040\n",
      "neoglia.learn.learner - INFO - \t- accuracy: 0.1872\n",
      "neoglia.learn.learner - INFO - h3: Test set: Average loss: 0.0107\n",
      "neoglia.learn.learner - INFO - \t- accuracy: 0.1370\n",
      "neoglia.learn.learner - INFO - Federated model: Test set: Average loss: 0.0008\n",
      "neoglia.learn.learner - INFO - \t- accuracy: 0.2108\n"
     ]
    }
   ],
   "source": [
    "fed_learner.train_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for worker in (h1, h2, h3):\n",
    "    worker.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained federated model\n",
    "\n",
    "And test it locally on a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.weight',\n",
       "              tensor([[[[ 1.8860e-02, -1.0073e-01, -1.0674e-01,  9.7052e-03, -1.5782e-01],\n",
       "                        [ 2.8249e-03,  6.3920e-03, -1.7211e-01,  9.3360e-02, -1.1974e-01],\n",
       "                        [ 1.3968e-02, -1.1998e-01, -1.4551e-01,  7.7240e-02, -7.2984e-02],\n",
       "                        [-2.7283e-02,  2.0283e-01,  1.5646e-01,  2.4561e-01,  1.5579e-01],\n",
       "                        [-1.5279e-01, -1.0797e-01,  7.7242e-02, -6.1008e-02,  2.3347e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.6632e-02, -5.6882e-02, -1.1255e-01, -1.1662e-01,  1.5993e-01],\n",
       "                        [ 1.3329e-01, -2.0634e-01, -1.1618e-01,  1.7164e-01,  1.2429e-01],\n",
       "                        [-1.7148e-01, -1.3992e-01, -7.1745e-02, -2.8118e-03,  2.0056e-01],\n",
       "                        [-1.9243e-03, -9.0390e-02, -1.9782e-01,  1.2490e-01,  2.0530e-01],\n",
       "                        [-1.1707e-01, -3.7539e-02, -2.7524e-02,  1.5139e-01,  2.2522e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.7271e-01, -6.7558e-02,  1.4662e-01, -1.9389e-01, -6.9102e-02],\n",
       "                        [ 1.8937e-01,  1.0744e-01, -1.0389e-01,  3.7357e-02,  1.0699e-01],\n",
       "                        [-1.5264e-01,  1.9943e-01, -1.9381e-01, -1.7772e-02, -1.7966e-01],\n",
       "                        [ 1.9237e-01, -1.8711e-01, -1.7797e-01, -5.7293e-02, -1.4259e-01],\n",
       "                        [ 1.5639e-01,  9.8542e-03, -1.0153e-01, -4.9253e-02,  1.8427e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.0014e-01, -1.2333e-01,  1.0360e-02, -2.0761e-02,  2.4027e-01],\n",
       "                        [-2.3961e-02,  1.4662e-01,  2.3252e-01,  1.1220e-01,  2.0290e-01],\n",
       "                        [-1.6821e-01, -1.9087e-01, -1.7978e-01,  2.6474e-02, -9.0656e-02],\n",
       "                        [-9.9148e-03, -1.8794e-01,  3.4499e-02, -1.8810e-01, -8.7670e-02],\n",
       "                        [ 6.5395e-02, -7.0892e-02, -3.2898e-02, -7.0960e-02, -1.9987e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.6529e-01,  1.2439e-02,  2.0214e-01,  1.0031e-01, -6.3983e-02],\n",
       "                        [-8.8270e-02,  1.9649e-01,  1.8490e-01, -1.2349e-01, -1.2909e-01],\n",
       "                        [-1.2267e-02,  1.8756e-01,  1.3361e-01,  7.2690e-02,  1.2926e-01],\n",
       "                        [-3.3193e-02, -1.2786e-01,  2.0416e-01, -1.8508e-01,  3.3304e-02],\n",
       "                        [ 9.2994e-02,  3.7923e-02, -1.2469e-01, -6.9508e-02, -1.4023e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.4179e-02, -8.6781e-03, -1.7746e-01, -1.9010e-01, -1.1489e-01],\n",
       "                        [ 6.6477e-02,  1.5501e-02, -1.5974e-01, -1.8666e-01,  1.6224e-01],\n",
       "                        [ 8.0223e-02, -6.9087e-02,  1.0162e-01,  1.3698e-01, -4.7411e-02],\n",
       "                        [ 1.5297e-01, -5.3033e-02, -9.4766e-02, -1.0990e-01, -1.1545e-02],\n",
       "                        [-9.7814e-02, -7.0049e-02, -1.2067e-01, -1.8886e-01, -1.7466e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.0991e-02, -1.9395e-01, -9.3490e-02, -1.4596e-01, -1.5876e-01],\n",
       "                        [ 1.4601e-03,  2.7832e-02, -9.0529e-02, -6.0070e-02,  7.6267e-02],\n",
       "                        [-4.9258e-02, -3.4582e-02, -1.0079e-01, -1.0404e-01, -1.7077e-01],\n",
       "                        [ 4.5686e-02,  8.2348e-02, -1.0040e-01,  3.0853e-02,  1.2950e-01],\n",
       "                        [-4.2722e-02, -9.9427e-02, -2.0070e-01,  7.7924e-02,  9.7614e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.1635e-02,  1.0381e-01, -1.2207e-01, -8.8708e-02,  1.2143e-01],\n",
       "                        [-7.1616e-02, -1.9558e-01, -1.9517e-01, -1.0246e-01, -1.5945e-01],\n",
       "                        [ 1.5584e-01, -5.8248e-02,  1.2312e-01, -1.1117e-02, -2.7833e-02],\n",
       "                        [-1.1175e-01, -1.4533e-02, -1.6434e-02,  1.0035e-01, -1.9283e-01],\n",
       "                        [-1.1821e-01, -3.1916e-02, -4.4903e-02,  1.7941e-01, -1.7667e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.3653e-02,  2.0915e-02,  2.2429e-01,  7.0776e-02,  1.2347e-01],\n",
       "                        [ 3.5046e-02, -9.9079e-02,  2.0311e-01,  1.6540e-01,  1.2789e-01],\n",
       "                        [-9.7485e-02,  1.4407e-01, -1.8738e-01, -2.8552e-03,  5.4172e-02],\n",
       "                        [ 1.3269e-01, -7.4814e-02, -1.0440e-01,  3.2698e-02, -1.7024e-01],\n",
       "                        [-2.8058e-02, -1.7359e-01,  9.3364e-02, -1.6140e-01, -1.1055e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.2794e-01,  2.9456e-02, -2.0076e-01, -1.6456e-01,  5.3905e-02],\n",
       "                        [ 1.1183e-01, -1.2818e-01,  1.2347e-01, -1.3927e-01, -1.8301e-02],\n",
       "                        [ 6.8009e-02,  2.2813e-02,  1.6513e-01,  1.2853e-01, -1.5197e-01],\n",
       "                        [ 9.7820e-02, -1.6563e-01, -1.5596e-01,  1.8456e-01, -7.5569e-02],\n",
       "                        [-4.8442e-02,  1.9821e-01,  1.2893e-01, -1.7078e-01,  7.5177e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.3690e-02, -1.9315e-01,  1.4969e-01,  5.8731e-02, -1.6474e-01],\n",
       "                        [ 5.9616e-03,  1.5503e-01,  2.0357e-02, -1.6574e-01, -8.1116e-02],\n",
       "                        [ 4.5604e-02, -1.6609e-01, -5.5178e-02, -9.3544e-02, -8.3905e-02],\n",
       "                        [-3.9677e-02, -4.9190e-02, -1.4072e-02, -9.1381e-03,  7.6977e-02],\n",
       "                        [ 7.1409e-03,  1.0564e-01,  1.5798e-01, -2.0347e-01, -1.6824e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.1771e-02, -1.3350e-01, -3.6992e-03,  9.4287e-02, -1.9208e-01],\n",
       "                        [ 2.0178e-01, -1.2658e-01, -5.4833e-02, -9.1519e-02, -1.5142e-01],\n",
       "                        [ 8.3033e-02,  1.6044e-01,  1.8619e-01, -1.2162e-01, -5.0260e-03],\n",
       "                        [-1.2029e-01, -1.5032e-01,  1.3071e-01, -1.4414e-01, -1.9519e-01],\n",
       "                        [-1.8199e-01,  2.1385e-01,  7.5132e-02, -1.0565e-01, -3.2522e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.1737e-01, -1.2849e-01,  5.8338e-02,  1.8918e-01,  9.8537e-02],\n",
       "                        [ 6.1626e-02,  6.3659e-02, -8.8143e-02,  9.9220e-02,  1.9936e-01],\n",
       "                        [ 1.0644e-01, -1.1162e-02,  1.6985e-01,  2.0348e-01,  1.6597e-01],\n",
       "                        [-9.0755e-02, -1.6397e-01, -1.2208e-01,  2.0274e-01,  1.0823e-01],\n",
       "                        [-1.5817e-01,  1.1688e-01, -1.5459e-01,  2.0139e-01, -1.3208e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.2855e-02, -1.0961e-01, -1.4203e-01,  2.4002e-01,  9.9155e-02],\n",
       "                        [ 1.6656e-01,  9.1285e-02,  1.5723e-03,  1.7733e-01,  2.3906e-01],\n",
       "                        [ 3.3592e-02, -8.0106e-03, -1.6020e-01, -7.1043e-02,  1.0475e-01],\n",
       "                        [-1.7993e-01,  2.3181e-04, -8.4145e-03,  1.5595e-01, -1.6403e-01],\n",
       "                        [-2.1499e-01,  4.4511e-02, -1.3326e-01,  1.0822e-01, -1.4578e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.7884e-03, -2.0149e-01, -1.1093e-01,  1.1569e-01,  8.9749e-03],\n",
       "                        [-6.7179e-02,  1.3444e-01, -4.7883e-02,  1.7947e-01, -1.2744e-01],\n",
       "                        [ 2.3170e-02, -2.9665e-02,  9.4701e-02, -9.0296e-02, -1.1774e-02],\n",
       "                        [ 8.8991e-02, -1.3710e-01, -7.4063e-02,  5.3702e-02,  1.9684e-01],\n",
       "                        [-7.8933e-02,  1.9697e-01,  1.4305e-01,  2.4778e-01,  2.1838e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.2787e-01,  1.8923e-01,  1.7879e-01,  2.7264e-01,  1.9132e-03],\n",
       "                        [-4.7953e-02,  1.6550e-01,  5.8784e-02, -1.1764e-01,  1.7513e-01],\n",
       "                        [-1.6007e-01, -1.4194e-01, -1.7742e-01,  1.6018e-01,  1.6460e-02],\n",
       "                        [-2.1552e-01, -2.4758e-01, -4.4803e-02, -1.8526e-01, -2.1189e-01],\n",
       "                        [-3.0300e-02,  8.7756e-02, -7.7448e-02, -8.2444e-02, -9.3310e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.4233e-01, -1.1228e-01, -9.7626e-02,  8.2583e-02, -7.9471e-02],\n",
       "                        [ 2.0280e-01,  5.3672e-02, -6.7708e-02,  1.5554e-01,  2.0607e-02],\n",
       "                        [ 2.5176e-01, -1.1544e-03,  9.1560e-02, -1.0508e-02, -2.0478e-01],\n",
       "                        [ 1.7769e-01,  2.6032e-01,  1.4170e-01, -7.9049e-02, -1.7558e-01],\n",
       "                        [ 2.0694e-01,  1.5408e-01, -8.0228e-03, -1.4449e-01, -1.4053e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.0313e-02, -1.4909e-01,  2.0776e-01, -1.6031e-01,  1.3702e-01],\n",
       "                        [ 1.6913e-01, -1.7188e-02,  1.7881e-01,  1.5023e-01,  3.7437e-02],\n",
       "                        [-1.5947e-01,  1.0171e-01, -2.6170e-03, -1.7610e-01,  1.0029e-01],\n",
       "                        [-1.5724e-01, -2.1301e-01, -3.1620e-02, -9.9027e-02,  1.5955e-01],\n",
       "                        [-1.5594e-03,  8.3161e-02, -1.1149e-01, -8.7243e-02, -7.0449e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.8761e-02, -1.5619e-01,  1.5341e-01,  2.0824e-02, -6.6170e-02],\n",
       "                        [-2.1862e-02, -1.1353e-02,  1.3163e-02, -1.4739e-01, -8.4033e-03],\n",
       "                        [ 2.3805e-01, -3.2159e-02, -1.5366e-01, -1.6426e-01,  1.2944e-01],\n",
       "                        [ 2.3274e-01,  3.4413e-02, -1.5515e-01, -6.4394e-02,  1.8997e-01],\n",
       "                        [ 1.8440e-01, -1.7372e-01,  9.3387e-02, -1.1863e-01, -5.4299e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.1782e-01,  2.1549e-01, -7.8344e-02, -4.3349e-02,  5.9369e-02],\n",
       "                        [-1.8328e-02,  2.5091e-01,  2.6378e-01,  1.4945e-01,  2.4507e-01],\n",
       "                        [-1.0279e-01,  9.0602e-02,  1.2051e-01,  2.1863e-01,  3.6825e-02],\n",
       "                        [-1.1829e-02, -5.2364e-02, -1.6112e-01, -1.9657e-02,  1.0964e-01],\n",
       "                        [-3.2331e-01, -1.5985e-01, -1.9369e-01, -2.4681e-01, -4.5218e-02]]]])),\n",
       "             ('conv1.bias',\n",
       "              tensor([-0.1273, -0.0315,  0.0808, -0.1777, -0.0991,  0.0586, -0.1220, -0.0361,\n",
       "                      -0.0722, -0.0504,  0.1905, -0.1060,  0.0211,  0.1004,  0.0276,  0.1035,\n",
       "                       0.0600, -0.0535, -0.0683,  0.0299])),\n",
       "             ('conv2.weight',\n",
       "              tensor([[[[-2.0309e-03, -2.4568e-02, -2.8490e-02, -2.9767e-02, -2.7190e-02],\n",
       "                        [ 3.8261e-03,  1.4352e-02, -8.3430e-03,  4.1515e-02, -4.1635e-02],\n",
       "                        [ 1.7019e-02, -1.3300e-02, -3.8453e-02,  4.2350e-02, -1.7918e-02],\n",
       "                        [ 3.6655e-02,  4.0179e-02,  3.1928e-02, -1.0579e-02, -1.6705e-02],\n",
       "                        [ 3.4937e-02,  2.8416e-02, -2.3711e-02, -4.6722e-04,  1.3319e-02]],\n",
       "              \n",
       "                       [[ 3.6942e-03, -4.8102e-02,  2.3065e-02,  1.3479e-02,  3.2198e-02],\n",
       "                        [-3.2571e-02,  3.5848e-02, -3.2327e-02,  3.6907e-02, -1.9242e-02],\n",
       "                        [-4.8696e-02, -8.7370e-04,  3.9198e-03, -3.3840e-02, -6.9479e-03],\n",
       "                        [ 2.0730e-02, -1.0303e-02, -1.1409e-02, -2.5346e-02,  4.1515e-02],\n",
       "                        [-6.3430e-03, -8.6215e-03,  4.3472e-03, -3.2756e-02, -4.6542e-02]],\n",
       "              \n",
       "                       [[-8.0029e-03, -3.8440e-02, -1.2139e-02,  1.5826e-02, -2.8418e-02],\n",
       "                        [-9.7223e-03, -1.0185e-02, -7.5221e-03,  2.9998e-04, -3.1003e-02],\n",
       "                        [ 2.8762e-02,  1.0712e-03,  9.4929e-03,  2.3649e-02, -9.3343e-03],\n",
       "                        [-1.4674e-03, -3.2598e-02, -2.5243e-03, -2.7789e-02,  3.3270e-02],\n",
       "                        [-1.6546e-02,  3.2579e-02,  2.5577e-02, -3.6802e-02,  3.9369e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.7640e-02,  2.5393e-03,  2.1559e-03, -3.3551e-02,  1.2365e-02],\n",
       "                        [ 4.1272e-02, -2.6316e-02, -4.1223e-02, -3.6948e-02,  2.9424e-02],\n",
       "                        [ 2.7374e-02, -1.7029e-02, -2.8872e-02,  1.7262e-02, -8.4135e-03],\n",
       "                        [-3.1455e-02, -8.7889e-03, -2.4114e-02,  2.2186e-02, -3.1077e-02],\n",
       "                        [ 1.9061e-02,  9.0810e-03,  6.6547e-03, -2.0259e-02,  3.1533e-02]],\n",
       "              \n",
       "                       [[ 2.2884e-03, -7.7949e-03,  7.8600e-03, -6.9843e-03, -1.6619e-02],\n",
       "                        [ 4.1021e-02, -6.9901e-03,  4.1467e-02,  3.9404e-02, -1.2227e-02],\n",
       "                        [-4.2068e-02, -4.2867e-02,  7.4315e-03, -5.0619e-03,  4.4339e-02],\n",
       "                        [ 7.5414e-03,  6.1273e-03,  2.5577e-02, -3.6198e-02,  3.2898e-02],\n",
       "                        [-1.2286e-02,  3.2098e-02, -2.0597e-02,  1.5954e-02, -4.8527e-03]],\n",
       "              \n",
       "                       [[ 5.1797e-03, -9.4989e-03, -3.3787e-03, -4.2326e-02, -9.2680e-03],\n",
       "                        [ 7.1386e-03, -2.5306e-02,  3.4758e-02, -3.1183e-02,  2.8056e-02],\n",
       "                        [-2.3410e-02,  3.2522e-03,  1.0639e-02,  2.9415e-02, -3.8107e-02],\n",
       "                        [-3.3962e-02, -1.2355e-02, -1.3064e-02,  2.0988e-02, -3.0632e-03],\n",
       "                        [-8.6450e-03, -4.8502e-02, -3.9354e-02,  4.0659e-02, -2.6300e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.3234e-04,  3.1298e-02, -3.0376e-02, -1.3798e-02,  1.7430e-02],\n",
       "                        [ 3.4286e-02,  1.5907e-02,  2.9445e-03,  1.2565e-02,  1.2094e-03],\n",
       "                        [ 2.7765e-02,  5.0601e-02, -9.8847e-03, -1.8196e-02, -2.6584e-02],\n",
       "                        [ 2.9256e-02, -2.8793e-02,  3.2334e-02, -2.1073e-02, -2.2698e-02],\n",
       "                        [ 3.1048e-02, -5.7728e-03, -2.2416e-02, -2.8652e-02, -1.4999e-03]],\n",
       "              \n",
       "                       [[-2.7848e-02,  1.4564e-02,  5.1397e-02, -3.7529e-02,  1.6560e-02],\n",
       "                        [-1.1849e-02,  3.8634e-02,  5.2516e-02,  3.3546e-02, -2.3485e-02],\n",
       "                        [ 2.7224e-02,  5.5930e-02,  2.8246e-02,  9.2191e-03, -1.0423e-02],\n",
       "                        [ 6.2162e-02,  3.1360e-02, -3.9922e-02,  3.1274e-04, -1.0053e-02],\n",
       "                        [ 6.7084e-02, -1.3059e-02,  1.9516e-02,  1.9829e-02, -4.2717e-02]],\n",
       "              \n",
       "                       [[ 1.5569e-02, -1.6487e-02, -1.1612e-03, -2.2784e-02,  3.7921e-02],\n",
       "                        [-2.8644e-02,  5.0478e-03,  1.5465e-02, -4.0654e-02, -1.3648e-03],\n",
       "                        [-3.0307e-02, -3.4312e-02,  1.5517e-02,  1.3216e-02, -1.1848e-02],\n",
       "                        [ 8.6350e-03, -4.0355e-03, -2.2095e-02,  4.5207e-02, -1.8025e-02],\n",
       "                        [ 3.9803e-02, -3.3925e-02, -2.8575e-02,  2.2887e-02, -4.9004e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.1020e-02,  3.3254e-02, -4.3987e-02,  3.1782e-02, -3.5744e-02],\n",
       "                        [ 4.0223e-02,  3.8571e-02, -2.6562e-03,  2.7065e-02, -5.6537e-03],\n",
       "                        [-3.7399e-02,  2.4872e-02, -1.9083e-03, -3.9637e-02,  3.5447e-02],\n",
       "                        [ 1.8841e-03, -4.2778e-02, -3.0255e-02,  4.1159e-02,  1.2877e-02],\n",
       "                        [-4.2537e-02,  1.4702e-02, -2.6862e-02, -3.3482e-02, -1.2202e-02]],\n",
       "              \n",
       "                       [[ 2.4192e-02, -1.9860e-03, -3.5499e-02,  4.0224e-02, -3.2800e-02],\n",
       "                        [-4.0431e-02,  1.9210e-02, -1.2497e-02, -3.6728e-02, -1.9354e-02],\n",
       "                        [-4.1358e-02, -8.9808e-03,  5.7184e-03, -3.9556e-02, -3.3123e-02],\n",
       "                        [-1.2145e-02, -4.9686e-03,  2.7893e-02, -1.3237e-02,  3.4315e-02],\n",
       "                        [-2.0057e-02, -1.4675e-02,  1.5851e-02, -6.6666e-03,  6.1944e-03]],\n",
       "              \n",
       "                       [[-1.7465e-02, -2.5047e-02,  2.4633e-03, -4.4806e-02,  1.1982e-02],\n",
       "                        [ 8.9242e-03,  1.8841e-02, -6.3363e-03,  4.3651e-03, -5.8186e-03],\n",
       "                        [ 2.1473e-02, -4.1738e-02, -4.5476e-02, -1.3005e-02,  3.0410e-02],\n",
       "                        [-3.8226e-02, -1.1928e-02,  1.9215e-02,  1.3127e-02,  1.4609e-02],\n",
       "                        [ 1.9717e-02, -2.5949e-02,  3.4731e-02,  1.1517e-02, -1.4370e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.8190e-02,  3.4886e-02,  1.7722e-02,  4.4741e-02,  4.3912e-02],\n",
       "                        [-2.1219e-02,  2.9751e-02, -2.2999e-02, -7.2638e-03,  3.5378e-02],\n",
       "                        [-1.1871e-02, -1.8447e-02, -4.7537e-02,  2.0523e-02,  3.7102e-02],\n",
       "                        [-1.3296e-02,  3.5466e-02, -2.2968e-02, -7.5770e-03,  2.8163e-03],\n",
       "                        [-1.6719e-03,  8.6938e-03,  1.2027e-02,  2.7782e-02,  2.8877e-02]],\n",
       "              \n",
       "                       [[ 1.9633e-02,  5.3636e-03,  1.5555e-02,  4.1175e-02,  3.6649e-02],\n",
       "                        [ 2.5432e-02, -3.1253e-02, -7.2412e-04,  2.8074e-02,  3.2717e-02],\n",
       "                        [-2.2098e-03,  2.1017e-02, -5.4313e-02,  1.4283e-02,  4.3478e-05],\n",
       "                        [-5.2556e-02, -1.4277e-02,  1.4518e-02,  4.6431e-02,  3.4203e-02],\n",
       "                        [-4.4044e-02, -3.1559e-02,  2.8321e-02,  3.8309e-02,  1.9381e-02]],\n",
       "              \n",
       "                       [[-3.5730e-02, -1.8856e-02,  2.7248e-02,  2.4878e-02, -2.7260e-02],\n",
       "                        [-4.8188e-03, -4.3943e-03, -8.9594e-04, -4.2520e-02, -1.2210e-02],\n",
       "                        [ 2.9552e-02,  3.5242e-02, -2.2732e-02, -3.8446e-04,  2.6102e-03],\n",
       "                        [-3.3862e-02, -6.0901e-03, -4.1703e-02, -1.2676e-02, -1.8042e-02],\n",
       "                        [-2.3707e-03, -2.8077e-02, -4.0831e-02,  1.9804e-02,  3.1134e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.6582e-02,  1.1127e-02,  3.0223e-02, -2.1691e-02, -3.7294e-02],\n",
       "                        [ 3.9699e-02, -4.1404e-02,  4.0125e-02, -3.9963e-02, -3.6771e-02],\n",
       "                        [ 1.7996e-02,  4.1110e-02, -3.1035e-02,  1.5564e-02, -3.3259e-02],\n",
       "                        [-4.0420e-02, -1.1782e-02,  4.4281e-02, -3.4545e-02,  7.1244e-03],\n",
       "                        [-3.3150e-02,  1.4276e-02, -2.7243e-02,  3.7696e-02, -4.5770e-02]],\n",
       "              \n",
       "                       [[ 1.0157e-02,  4.3868e-02,  2.7399e-02,  2.1395e-03,  1.8378e-02],\n",
       "                        [-7.1191e-03, -2.2611e-02, -3.1523e-02, -2.5799e-02,  9.5087e-03],\n",
       "                        [-9.5847e-03,  1.0998e-02, -2.1554e-03, -9.5970e-03, -1.0719e-02],\n",
       "                        [-1.7336e-03, -4.9997e-02, -4.4653e-02,  2.8257e-02, -2.2011e-02],\n",
       "                        [-6.5241e-03, -3.1916e-02, -3.5604e-02, -1.6770e-02, -2.8317e-02]],\n",
       "              \n",
       "                       [[ 7.0149e-03, -4.3720e-02, -6.7325e-03,  2.9683e-02,  4.8335e-04],\n",
       "                        [ 2.9139e-02,  1.4904e-02,  3.4939e-03,  4.7486e-03, -3.0879e-02],\n",
       "                        [ 1.3162e-02,  1.0726e-02,  5.2565e-02,  4.6912e-02,  3.7945e-02],\n",
       "                        [-1.1222e-02, -1.9780e-02,  3.8901e-02, -1.1226e-02,  1.5201e-02],\n",
       "                        [ 2.6175e-05, -1.0859e-02, -5.9048e-03, -3.1741e-02,  9.5352e-03]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 2.7327e-03, -3.4495e-02, -6.4909e-03,  7.3470e-03, -1.8819e-02],\n",
       "                        [ 5.9370e-03,  7.4569e-03, -1.8107e-02,  7.2393e-03, -3.4299e-02],\n",
       "                        [ 3.5509e-02,  3.8908e-02,  2.9712e-03, -2.2520e-02, -2.5773e-02],\n",
       "                        [ 2.4083e-02, -1.1838e-02, -2.7677e-02,  1.3343e-02, -3.5257e-02],\n",
       "                        [-1.4615e-03, -1.7121e-02,  9.8645e-03, -3.9177e-03, -3.9687e-02]],\n",
       "              \n",
       "                       [[ 2.9549e-02, -4.8078e-03, -3.2667e-02, -4.5173e-02,  1.1095e-02],\n",
       "                        [-2.1791e-02,  6.3109e-03, -2.6202e-02, -3.9005e-03, -2.5538e-02],\n",
       "                        [-2.8827e-02,  2.5801e-02, -1.5302e-03, -9.1293e-03,  1.0059e-03],\n",
       "                        [ 2.3005e-02,  5.2508e-03,  4.4916e-02,  1.4425e-02,  4.8663e-03],\n",
       "                        [ 1.8995e-02, -1.7380e-03, -3.4691e-02,  6.1637e-03,  2.1653e-02]],\n",
       "              \n",
       "                       [[-1.7175e-02, -2.9014e-02, -3.1872e-02, -1.7022e-02, -9.1727e-03],\n",
       "                        [ 3.8178e-02, -2.9438e-02, -4.4842e-02,  2.4208e-02, -1.6220e-02],\n",
       "                        [ 1.4458e-02, -3.9314e-02, -4.3968e-02,  3.9308e-02, -1.0991e-02],\n",
       "                        [-1.9591e-02, -1.1097e-02,  2.1798e-02,  3.0341e-02,  5.9581e-03],\n",
       "                        [ 3.2959e-02, -1.9916e-02,  7.8685e-03, -4.2469e-02, -4.3848e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 4.2697e-03,  3.2463e-03, -1.9275e-02,  7.6473e-03,  2.8603e-03],\n",
       "                        [-2.7434e-02, -4.3480e-02, -2.9797e-02,  4.3949e-02,  1.8218e-02],\n",
       "                        [ 2.5650e-02,  1.1376e-02, -6.3340e-03, -9.9322e-03, -3.7228e-02],\n",
       "                        [-4.5277e-03, -3.5390e-02,  8.3226e-03, -3.1017e-02, -3.1168e-02],\n",
       "                        [ 1.8780e-02,  3.9770e-02, -4.4346e-02,  4.0525e-02,  7.9629e-03]],\n",
       "              \n",
       "                       [[ 3.1791e-02, -4.1427e-02, -1.5024e-02, -9.5879e-03,  4.4749e-02],\n",
       "                        [ 1.8542e-03,  7.1241e-03,  3.3927e-02,  2.0801e-02, -2.5927e-02],\n",
       "                        [ 2.2434e-02, -2.8526e-02,  4.0583e-02, -4.0154e-02, -3.6467e-02],\n",
       "                        [ 3.2411e-02,  4.4323e-02, -1.5901e-02,  1.6659e-02,  4.4905e-02],\n",
       "                        [ 5.6222e-03, -3.5489e-02, -2.6372e-02, -1.6222e-02, -1.8705e-02]],\n",
       "              \n",
       "                       [[ 1.9945e-02,  3.4891e-03, -2.0202e-02, -1.4354e-02, -1.7170e-02],\n",
       "                        [-1.9187e-03,  1.3729e-02,  1.7178e-02, -1.3013e-02, -1.9998e-02],\n",
       "                        [-3.3006e-02, -3.7593e-02,  1.8539e-02, -3.2937e-02,  4.5758e-03],\n",
       "                        [-2.1992e-02, -2.6790e-02,  1.3543e-02, -3.6464e-02,  4.2197e-02],\n",
       "                        [ 4.2896e-02,  3.0306e-02, -3.8192e-02, -1.0915e-03,  3.6156e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.4139e-02, -2.7481e-02, -1.9702e-02,  1.9860e-02,  1.7537e-02],\n",
       "                        [-3.1561e-02, -2.7793e-02,  3.9621e-02,  3.4724e-02, -2.3714e-02],\n",
       "                        [ 4.5103e-02,  3.1828e-02, -7.4622e-03,  1.0741e-02,  1.4843e-04],\n",
       "                        [-1.1505e-02, -5.3619e-02,  1.1095e-02,  2.2491e-02, -1.3573e-02],\n",
       "                        [ 2.5962e-02,  8.5845e-03,  1.1089e-02, -1.5885e-02,  1.6831e-03]],\n",
       "              \n",
       "                       [[-3.4607e-03, -3.5055e-02,  9.7109e-04,  4.3891e-02,  1.2533e-02],\n",
       "                        [ 3.0536e-02,  3.9548e-02, -2.8610e-02, -5.9262e-03,  9.0056e-03],\n",
       "                        [-3.6191e-02, -1.8331e-02, -4.9336e-02, -2.7993e-03, -4.1334e-02],\n",
       "                        [-2.1918e-02,  3.7931e-03, -5.8233e-02, -6.5888e-03, -1.8106e-02],\n",
       "                        [ 3.2617e-04, -2.3129e-02,  2.9753e-03, -2.5157e-02, -1.6284e-02]],\n",
       "              \n",
       "                       [[ 2.1730e-02,  2.7062e-02,  1.0481e-03,  3.3066e-02, -4.0665e-02],\n",
       "                        [ 3.9423e-02,  3.3548e-02, -1.7825e-02, -2.8205e-03, -4.7918e-02],\n",
       "                        [ 1.1062e-02,  3.7417e-02, -4.5685e-02, -2.1162e-02, -1.2347e-03],\n",
       "                        [-4.8781e-02,  3.9834e-02,  3.2724e-02, -3.6523e-02, -4.7577e-02],\n",
       "                        [-7.5660e-04, -2.9631e-02, -9.9843e-03,  9.4310e-03, -2.2533e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.2628e-03, -2.4792e-02, -4.0156e-02,  1.6151e-02,  4.1971e-02],\n",
       "                        [ 2.5285e-02, -6.9783e-03, -4.4355e-02,  3.0769e-03, -1.7478e-02],\n",
       "                        [-3.0606e-02, -1.7294e-02, -2.4203e-02, -1.9206e-02,  1.6250e-02],\n",
       "                        [-2.2607e-02,  3.8288e-02, -2.8575e-03,  1.7953e-02, -1.7724e-02],\n",
       "                        [ 1.8717e-02,  1.6031e-02, -3.1189e-02, -3.7084e-02, -1.3707e-03]],\n",
       "              \n",
       "                       [[-3.2149e-02, -1.1542e-02,  2.5366e-02,  3.8306e-02,  2.2563e-02],\n",
       "                        [-3.5213e-02, -2.8473e-02,  1.1234e-02,  3.1535e-02,  1.8189e-02],\n",
       "                        [-4.1297e-02,  1.1635e-02, -2.9788e-02, -2.0271e-02,  1.6474e-02],\n",
       "                        [-3.7019e-02,  3.2625e-02,  3.7368e-03, -3.8266e-02, -1.2331e-02],\n",
       "                        [-2.6236e-02, -1.0724e-02, -4.6865e-02, -2.5735e-03, -1.5189e-02]],\n",
       "              \n",
       "                       [[-1.3815e-02,  3.0759e-02, -6.7153e-03, -1.1857e-02,  5.0356e-03],\n",
       "                        [-2.0782e-02,  2.2375e-02, -2.9185e-02,  2.1598e-02,  4.8520e-03],\n",
       "                        [-1.2692e-02,  2.2533e-03, -1.2635e-02,  7.6434e-02,  7.5517e-02],\n",
       "                        [ 3.2002e-02,  4.1264e-02,  9.3722e-03,  2.2868e-02,  6.9136e-02],\n",
       "                        [ 5.4800e-02,  1.4547e-02,  2.8409e-02, -6.3164e-02, -3.3658e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.9426e-02,  2.2076e-02, -1.6543e-02, -3.7188e-02,  1.8146e-02],\n",
       "                        [-4.5217e-02, -1.2927e-02, -3.0625e-02,  2.3860e-02, -7.6332e-03],\n",
       "                        [ 1.0791e-02,  2.6460e-02, -3.0633e-03, -3.0985e-03,  7.6870e-03],\n",
       "                        [ 3.7576e-03,  3.7579e-02, -2.9278e-02,  3.4505e-02,  1.0121e-02],\n",
       "                        [ 1.9369e-02,  3.4603e-02,  4.1945e-02,  2.7314e-02, -1.6369e-02]],\n",
       "              \n",
       "                       [[-2.5032e-02, -2.7086e-02, -1.0102e-02,  5.9110e-02,  7.1642e-02],\n",
       "                        [-4.6469e-02, -5.4754e-03, -2.3121e-02,  6.1426e-02,  2.3130e-03],\n",
       "                        [-1.0625e-02,  1.2741e-02,  4.6511e-02,  8.8418e-02,  2.0684e-02],\n",
       "                        [-2.6153e-02, -7.5332e-04,  5.0023e-02,  6.4864e-02, -2.0184e-02],\n",
       "                        [ 3.4821e-02, -3.5998e-02,  5.6121e-02,  1.3285e-02, -4.9292e-02]],\n",
       "              \n",
       "                       [[-2.0753e-02,  2.8354e-02,  3.0633e-02,  2.2776e-02,  2.6629e-02],\n",
       "                        [ 8.1701e-03,  2.7640e-03,  2.4682e-02, -2.1245e-02,  1.1575e-02],\n",
       "                        [-2.1375e-02,  4.3696e-02, -2.7306e-02, -6.2341e-04, -3.4561e-02],\n",
       "                        [-1.4456e-02,  9.7692e-03, -3.9393e-02, -4.7202e-02,  3.0278e-02],\n",
       "                        [-1.3558e-02, -1.6836e-02,  1.3236e-02,  3.5757e-02,  3.4845e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.2947e-02, -1.5440e-02,  4.3664e-02,  3.9555e-02, -2.0186e-02],\n",
       "                        [ 4.3960e-03,  9.1713e-03,  5.6310e-03,  4.0767e-02, -3.5557e-02],\n",
       "                        [ 2.0754e-02, -2.1274e-02, -1.2138e-04,  1.6596e-02,  3.3764e-02],\n",
       "                        [-1.2327e-02, -6.4571e-03, -1.4016e-02, -3.6122e-02,  3.8282e-02],\n",
       "                        [ 1.4270e-02,  2.8118e-02,  1.2916e-02, -1.8256e-02, -2.6881e-03]],\n",
       "              \n",
       "                       [[-3.9849e-03,  1.8640e-02, -5.3238e-03, -2.8998e-02, -7.0303e-03],\n",
       "                        [-2.6201e-02,  3.7017e-02,  2.5052e-02,  4.6510e-03,  2.8686e-03],\n",
       "                        [-2.5518e-02, -3.3145e-03,  3.2677e-02, -2.3919e-02,  2.5708e-02],\n",
       "                        [ 2.2876e-02,  4.2862e-02,  1.2149e-02,  2.0776e-02,  2.7337e-02],\n",
       "                        [ 3.7699e-02,  1.8689e-02,  4.3164e-02, -3.4627e-02,  2.8389e-02]],\n",
       "              \n",
       "                       [[ 3.0932e-02, -2.4535e-02, -2.8891e-02,  2.1246e-02,  2.0866e-02],\n",
       "                        [-2.9940e-02, -9.9009e-03, -5.7652e-03,  1.4250e-02, -3.2891e-02],\n",
       "                        [-2.9033e-02, -2.9748e-02,  2.1447e-02, -4.8700e-02, -3.3255e-02],\n",
       "                        [ 2.5493e-02, -4.0738e-02, -3.8230e-02, -2.1226e-02, -3.9809e-02],\n",
       "                        [ 3.2413e-02,  1.9502e-02, -3.6036e-02, -2.6347e-02,  1.2645e-02]]]])),\n",
       "             ('conv2.bias',\n",
       "              tensor([ 0.0437, -0.0454, -0.0014, -0.0165,  0.0094,  0.0375,  0.0163,  0.0162,\n",
       "                      -0.0186, -0.0037, -0.0392,  0.0036,  0.0090,  0.0300,  0.0072, -0.0172,\n",
       "                      -0.0339, -0.0404,  0.0094, -0.0327, -0.0341, -0.0219,  0.0138, -0.0284,\n",
       "                       0.0167, -0.0197,  0.0263, -0.0018,  0.0271, -0.0043, -0.0332,  0.0168,\n",
       "                       0.0300,  0.0382, -0.0237,  0.0274,  0.0194,  0.0271,  0.0103, -0.0318,\n",
       "                      -0.0142,  0.0463,  0.0448, -0.0148,  0.0493,  0.0353,  0.0218, -0.0057,\n",
       "                       0.0156,  0.0131])),\n",
       "             ('fc1.weight',\n",
       "              tensor([[ 0.0355, -0.0159, -0.0053,  ..., -0.0070,  0.0198, -0.0302],\n",
       "                      [ 0.0008,  0.0091,  0.0337,  ...,  0.0331, -0.0344,  0.0133],\n",
       "                      [ 0.0340,  0.0248, -0.0019,  ..., -0.0196,  0.0113,  0.0114],\n",
       "                      ...,\n",
       "                      [-0.0239, -0.0324, -0.0240,  ...,  0.0098, -0.0208, -0.0199],\n",
       "                      [-0.0115,  0.0033, -0.0118,  ..., -0.0242, -0.0252, -0.0135],\n",
       "                      [ 0.0161,  0.0120, -0.0219,  ...,  0.0213,  0.0324,  0.0171]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([ 0.0360,  0.0310,  0.0302, -0.0352, -0.0224,  0.0067,  0.0095,  0.0172,\n",
       "                       0.0380,  0.0042,  0.0301,  0.0095,  0.0249, -0.0086, -0.0209,  0.0052,\n",
       "                      -0.0135, -0.0341,  0.0061,  0.0213,  0.0246,  0.0275,  0.0067,  0.0256,\n",
       "                       0.0076,  0.0049, -0.0131, -0.0077, -0.0090, -0.0202,  0.0394, -0.0167,\n",
       "                       0.0131,  0.0011, -0.0307,  0.0092,  0.0149, -0.0022, -0.0117,  0.0230,\n",
       "                      -0.0273,  0.0025,  0.0173,  0.0365,  0.0357, -0.0175,  0.0099, -0.0129,\n",
       "                       0.0069,  0.0306,  0.0149, -0.0114, -0.0252, -0.0059,  0.0337, -0.0216,\n",
       "                       0.0154, -0.0041, -0.0176, -0.0127,  0.0113,  0.0067,  0.0172, -0.0316,\n",
       "                      -0.0184,  0.0287,  0.0146, -0.0180, -0.0099, -0.0266,  0.0161, -0.0289,\n",
       "                       0.0126, -0.0340, -0.0301, -0.0014,  0.0267, -0.0237,  0.0046, -0.0137,\n",
       "                      -0.0206,  0.0174, -0.0327,  0.0114,  0.0300, -0.0152,  0.0067, -0.0094,\n",
       "                      -0.0320, -0.0176,  0.0165, -0.0299,  0.0306, -0.0202,  0.0119,  0.0109,\n",
       "                       0.0280,  0.0348,  0.0168,  0.0002, -0.0035, -0.0139, -0.0226,  0.0156,\n",
       "                      -0.0234, -0.0085, -0.0173,  0.0238, -0.0314,  0.0158, -0.0087, -0.0237,\n",
       "                       0.0022, -0.0267,  0.0378, -0.0256, -0.0347, -0.0227, -0.0077,  0.0135,\n",
       "                      -0.0218,  0.0012, -0.0254, -0.0019,  0.0179, -0.0316,  0.0151,  0.0075,\n",
       "                      -0.0343, -0.0327,  0.0326,  0.0238, -0.0327,  0.0160, -0.0283,  0.0056,\n",
       "                       0.0022, -0.0222,  0.0132, -0.0288,  0.0027,  0.0205, -0.0126,  0.0056,\n",
       "                       0.0315,  0.0275,  0.0100,  0.0285, -0.0323,  0.0238,  0.0042, -0.0150,\n",
       "                      -0.0002,  0.0222,  0.0233,  0.0327,  0.0119,  0.0255, -0.0091,  0.0325,\n",
       "                       0.0025, -0.0125,  0.0302,  0.0040,  0.0132,  0.0162, -0.0293, -0.0214,\n",
       "                      -0.0125, -0.0300,  0.0143, -0.0230,  0.0033, -0.0294, -0.0325,  0.0411,\n",
       "                       0.0105,  0.0034,  0.0041, -0.0016, -0.0179,  0.0281,  0.0019,  0.0182,\n",
       "                      -0.0071,  0.0273,  0.0220, -0.0176,  0.0326, -0.0283,  0.0313,  0.0089,\n",
       "                      -0.0118, -0.0174,  0.0307, -0.0112, -0.0038, -0.0121,  0.0323, -0.0203,\n",
       "                       0.0221,  0.0377, -0.0124, -0.0160,  0.0251,  0.0114,  0.0319, -0.0192,\n",
       "                       0.0256,  0.0167, -0.0128, -0.0294, -0.0251, -0.0020, -0.0047,  0.0316,\n",
       "                      -0.0288,  0.0064,  0.0024, -0.0077, -0.0079,  0.0333,  0.0110,  0.0299,\n",
       "                       0.0144,  0.0172,  0.0319,  0.0304, -0.0048,  0.0107,  0.0042,  0.0249,\n",
       "                      -0.0125, -0.0033,  0.0255, -0.0060, -0.0303, -0.0304, -0.0339,  0.0325,\n",
       "                       0.0203, -0.0237,  0.0213,  0.0272, -0.0101,  0.0080,  0.0303,  0.0341,\n",
       "                       0.0016,  0.0080, -0.0102, -0.0269,  0.0340, -0.0152, -0.0331, -0.0107,\n",
       "                       0.0211,  0.0222, -0.0295,  0.0106,  0.0205, -0.0280,  0.0369,  0.0211,\n",
       "                      -0.0005,  0.0376, -0.0307,  0.0075, -0.0221, -0.0098, -0.0170, -0.0291,\n",
       "                       0.0412, -0.0087, -0.0158, -0.0020,  0.0238,  0.0158,  0.0014,  0.0312,\n",
       "                      -0.0212,  0.0028,  0.0275, -0.0325, -0.0080, -0.0132,  0.0177, -0.0314,\n",
       "                      -0.0286, -0.0342,  0.0260, -0.0252,  0.0323, -0.0112,  0.0002,  0.0186,\n",
       "                      -0.0259, -0.0277,  0.0389, -0.0226, -0.0259, -0.0101,  0.0028, -0.0176,\n",
       "                       0.0326, -0.0300, -0.0322,  0.0158,  0.0077, -0.0089, -0.0317, -0.0020,\n",
       "                      -0.0134,  0.0071, -0.0178,  0.0249,  0.0231, -0.0203, -0.0109,  0.0066,\n",
       "                       0.0072,  0.0192,  0.0100, -0.0363,  0.0227,  0.0140,  0.0030, -0.0248,\n",
       "                       0.0239,  0.0011,  0.0179, -0.0138,  0.0331,  0.0183, -0.0284, -0.0255,\n",
       "                       0.0166, -0.0344,  0.0239, -0.0020,  0.0225, -0.0079,  0.0129, -0.0151,\n",
       "                      -0.0059, -0.0061,  0.0138, -0.0093,  0.0230, -0.0310, -0.0168,  0.0190,\n",
       "                       0.0182,  0.0179, -0.0028, -0.0085, -0.0046, -0.0004, -0.0011, -0.0184,\n",
       "                       0.0311, -0.0216, -0.0135,  0.0332, -0.0215,  0.0054, -0.0060,  0.0315,\n",
       "                      -0.0345, -0.0095,  0.0120, -0.0299,  0.0191,  0.0097, -0.0240,  0.0129,\n",
       "                       0.0380,  0.0179,  0.0093, -0.0122, -0.0298, -0.0065,  0.0132,  0.0227,\n",
       "                       0.0048, -0.0097, -0.0304,  0.0266, -0.0201,  0.0046,  0.0226, -0.0215,\n",
       "                      -0.0336,  0.0227,  0.0315,  0.0078,  0.0342,  0.0344, -0.0291,  0.0138,\n",
       "                       0.0255, -0.0167, -0.0260, -0.0034,  0.0249, -0.0193,  0.0218, -0.0329,\n",
       "                      -0.0179,  0.0199,  0.0120, -0.0234,  0.0167,  0.0088, -0.0219, -0.0126,\n",
       "                       0.0282, -0.0160,  0.0063,  0.0048,  0.0351, -0.0068,  0.0135,  0.0339,\n",
       "                      -0.0342, -0.0309, -0.0267,  0.0158, -0.0289, -0.0275,  0.0302,  0.0263,\n",
       "                      -0.0027, -0.0215, -0.0028, -0.0307, -0.0192, -0.0085,  0.0111,  0.0171,\n",
       "                      -0.0001,  0.0025,  0.0142, -0.0074,  0.0087, -0.0282, -0.0319, -0.0113,\n",
       "                      -0.0178,  0.0010, -0.0018,  0.0272,  0.0073, -0.0032,  0.0281,  0.0152,\n",
       "                      -0.0127, -0.0011, -0.0226, -0.0224,  0.0230,  0.0150,  0.0069,  0.0148,\n",
       "                       0.0043, -0.0120, -0.0080,  0.0199,  0.0035,  0.0339, -0.0152, -0.0244,\n",
       "                      -0.0101,  0.0305,  0.0362, -0.0017,  0.0330,  0.0148,  0.0008,  0.0210,\n",
       "                      -0.0248,  0.0057,  0.0221, -0.0298,  0.0285, -0.0066, -0.0074,  0.0191,\n",
       "                      -0.0136,  0.0131, -0.0079,  0.0431,  0.0139, -0.0275, -0.0157, -0.0012,\n",
       "                       0.0303, -0.0147,  0.0235, -0.0244])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[ 0.0112,  0.0174, -0.0150,  ...,  0.0025, -0.0231, -0.0328],\n",
       "                      [-0.0631, -0.0479, -0.0422,  ...,  0.0172,  0.0287, -0.0010],\n",
       "                      [-0.0627,  0.0091,  0.0003,  ..., -0.0293, -0.0418, -0.0262],\n",
       "                      ...,\n",
       "                      [ 0.0657,  0.0490,  0.0435,  ...,  0.0225, -0.0029,  0.0106],\n",
       "                      [ 0.0403, -0.0078, -0.0508,  ..., -0.0257, -0.0585, -0.0447],\n",
       "                      [ 0.1135,  0.0190, -0.0068,  ...,  0.0072, -0.0325, -0.0338]])),\n",
       "             ('fc2.bias',\n",
       "              tensor([-0.0169, -0.0341, -0.0642, -0.0418, -0.0273,  0.0505, -0.0062,  0.0319,\n",
       "                       0.0103,  0.1037]))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "m = torch.load('mnist_train_model.pt')\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
